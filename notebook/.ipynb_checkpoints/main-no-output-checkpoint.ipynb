{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pP6IUTe2S55I"
   },
   "outputs": [],
   "source": [
    "# !pip3 -qq install python_speech_features\n",
    "# !pip3 -qq install modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ibMyT31v7-Nb"
   },
   "outputs": [],
   "source": [
    "seed=42\n",
    "import plotly\n",
    "def plotlyC(x,d): # d to dim the color\n",
    "    return tuple(map(lambda x: min(int(x)+d*8,255)/255, plotly.colors.DEFAULT_PLOTLY_COLORS[x][4:-1].split(',')))\n",
    "import plotly.graph_objs as go\n",
    "from plotly.graph_objs import Layout\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from skimage import io\n",
    "from scipy.stats import entropy\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import rc, cm\n",
    "#import seaborn as sns\n",
    "import tensorflow as tf\n",
    "rc('font', size=12)\n",
    "from math import sqrt\n",
    "SPINE_COLOR = 'gray'\n",
    "\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=False)\n",
    "\n",
    "def format_axes(ax):\n",
    "\n",
    "    for spine in ['top', 'right']:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "\n",
    "    for spine in ['left', 'bottom']:\n",
    "        ax.spines[spine].set_color(SPINE_COLOR)\n",
    "        ax.spines[spine].set_linewidth(0.5)\n",
    "\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "    for axis in [ax.xaxis, ax.yaxis]:\n",
    "        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n",
    "\n",
    "    return ax\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify color scheme\n",
    "def hexify(r,g,b,a):\n",
    "    return '#'+hex(r)[2:]+hex(g)[2:]+hex(b)[2:]+hex(a)[2:]\n",
    "\n",
    "my_clr   = {'l_b':hexify(147,205,221,255),\n",
    "            'b':hexify(81,151,213,255),\n",
    "            'eq':hexify(224,224,224,255),\n",
    "            'y':hexify(253,215,42,255),\n",
    "            'l_r':hexify(248,177,99,255)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Active Learning: A Visual Tour</center></h1>\n",
    "<h4><center><a style=\"text-decoration:none\" href=\"https://patel-zeel.github.io/\">Zeel B Patel</a>, IIT Gandhinagar, <a style=\"text-decoration:none\" href=\"mailto:patel_zeel@iitgn.ac.in\">patel_zeel@iitgn.ac.in</a>\n",
    "     <br><br>\n",
    "            <a style=\"text-decoration:none\" href=\"https://nipunbatra.github.io/\">Nipun Batra</a>, IIT Gandhinagar, <a style=\"text-decoration:none\" href=\"mailto:nipun.batra@iitgn.ac.in\">nipun.batra@iitgn.ac.in</a> </center></h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ib7Zg78y7NMw"
   },
   "source": [
    "# Rise of Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2f2R73h7NMy"
   },
   "source": [
    "Today, machine learning (ML) is applied to numerous fields, including, but not limited to Natural Language Processing (<a style=\"text-decoration:none\" href=\"https://en.wikipedia.org/wiki/Natural_language_processing\">NLP</a>), <a style=\"text-decoration:none\" href=\"https://en.wikipedia.org/wiki/Computer-aided_diagnosis\">Computer-aided diagnosis</a>, <a style=\"text-decoration:none\" href=\"https://en.wikipedia.org/wiki/Mathematical_optimization\">Optimization</a>, and <a style=\"text-decoration:none\" href=\"https://books.google.co.in/books/about/Bioinformatics.html?id=pxSM7R1sdeQC&redir_esc=y\">Bioinformatics</a>. A significant proportion of this success is due to a subset of ML called supervised learning. There are three main reasons behind the success of supervised learning (and machine learning, generally): 1) availability of massive data; 2) better algorithms; and 3) powerful computational infrastructure jointly called the <a style=\"text-decoration:none\" href=\"https://simons.berkeley.edu/events/openlectures2018-fall-1#:~:text=AI%20at%20scale%20requires%20a,data%2C%20algorithms%20and%20cloud%20infrastructure.&text=By%20building%20intelligence%20into%20data,to%20train%20models%20at%20scale.\">AI Trinity</a>. Supervised learning techniques require *labeled* data. As the data turns into 'Big data,' the effort required to label it becomes more laborious. In this article, we will talk about active learning, a suite of techniques for intelligent and data-driven annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "eval('HTML(\"\"\"\\\n",
    "<style>\\\n",
    ".output_png {\\\n",
    "    display: table-cell;\\\n",
    "    text-align: center;\\\n",
    "    vertical-align: middle;\\\n",
    "}\\\n",
    "</style>\\\n",
    "\"\"\")')\n",
    "#clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WmXKpz_7NMz"
   },
   "source": [
    "# Data Annotation is Expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvuTF89uNIyB"
   },
   "source": [
    "Labeled data is the primary need of supervised learning. Annotating the data is expensive because it may require: i) excessive time and manual effort ii) costly sensors. Let us get an overview of how expensive labeling is with a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPpqMFzEOCJi"
   },
   "source": [
    "## Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xyp36YcFPxhD"
   },
   "source": [
    "Let us say we need to convert a speech or audio into text for an application such as subtitle generation (speech-to-text task). We have to annotate multiple audio segments that correspond to the words and phrases in the audio. The following example audio of 6 seconds may require around a minute to annotate manually. Thus, annotating hours of publicly available audio data is an impracticable task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRos10LF7NM1"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "_kR93ZjX7NM8",
    "outputId": "c09d2d02-6380-4cf4-8fa5-163786613863"
   },
   "outputs": [],
   "source": [
    "# aud1 = Audio(url='https://raw.githubusercontent.com/patel-zeel/OpenActiveLearning/master/Audio/demo.wav')\n",
    "aud1 = Audio(filename='../audio/demo.wav')\n",
    "#!wget https://raw.githubusercontent.com/patel-zeel/OpenActiveLearning/master/Audio/demo.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "cGu1_ebBPw5l",
    "outputId": "22241d3f-a0a0-4194-a811-2daff1fdce1a"
   },
   "outputs": [],
   "source": [
    "display(aud1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kU9TA1BJhItG"
   },
   "source": [
    "Similarly, researchers have made efforts to detect COVID-19 from the sound of the human cough [<a style=\"text-decoration:none\" href=\"https://www.sciencedirect.com/science/article/pii/S2352914820303026\">10</a>]. Collecting labels for thousands of human cough samples is a difficult task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SFnK3WmkODj"
   },
   "source": [
    "## Human Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDGOiDQbrHiA"
   },
   "source": [
    "Let us say we want to classify different human activities into different categories (shown in Figure 2). We need expensive sensors to monitor the alignment or motion of the human body parts for such tasks. In the end, we need to map the sensor data with various activities with substantial manual effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgw = io.imread('../images/walking.png')\n",
    "imgr = io.imread('../images/Running.png')\n",
    "imgs = io.imread('../images/Sitting.png')\n",
    "imgc = io.imread('../images/Climbing.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "colab_type": "code",
    "id": "ElwGHEgZhIcN",
    "outputId": "690ec2f0-4de6-4723-fdd8-bdd7c6e416c7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# fig, axes = plt.subplots(1,4, figsize=(16,4))\n",
    "# for ax in axes:\n",
    "#     ax.set_axis_off()\n",
    "# axes[0].imshow(Image.open('../images/walking.png').resize((220,250)))\n",
    "# axes[0].set_title('Walking')\n",
    "# axes[1].imshow(Image.open('../images/Running.png').resize((220,250)))\n",
    "# axes[1].set_title('Running')\n",
    "# axes[2].imshow(Image.open('../images/Sitting.png').resize((220,250)));\n",
    "# axes[2].set_title('Sitting')\n",
    "# axes[3].imshow(Image.open('../images/Climbing.png').resize((220,250)));\n",
    "# axes[3].set_title('Climbing')\n",
    "# plt.gcf().set_facecolor(\"white\");\n",
    "# plt.figtext(0.4,1,'Figure 1: Various human activities',fontdict={'size':16});\n",
    "# init = time()\n",
    "# Create traces\n",
    "fig = make_subplots(1, 4, subplot_titles=['Walking','Running','Sitting','Climbing'])\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "fig.update_xaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
    "fig.update_layout(title_text='<b>Figure 1:</b> Various human activities', \n",
    "                  title_x=0.5,height=370,hovermode=False\n",
    "    )\n",
    "for ind, img in enumerate(['../images/walking.png','../images/Running.png',\n",
    "                           '../images/Sitting.png','../images/Climbing.png'],1):\n",
    "    fig.add_layout_image(\n",
    "    source=Image.open(img),\n",
    "    xref=\"x\",\n",
    "    yref=\"y\",\n",
    "    x=-1,\n",
    "    sizex=7,\n",
    "    y=4,\n",
    "    sizey=5,\n",
    "    opacity=1,\n",
    "    layer=\"below\",\n",
    "    sizing=\"stretch\",\n",
    "    row=1, col=ind)\n",
    "\n",
    "#print(time()-init)\n",
    "# fig.layout.annotations[0].update(y=0.78)\n",
    "# fig.layout.annotations[1].update(y=0.78)\n",
    "# fig.layout.annotations[2].update(y=0.78)\n",
    "# fig.layout.annotations[3].update(y=0.78)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdWLeJc77NNE"
   },
   "source": [
    "# All the Samples are Not Equally Important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FABxbhWi7NNF"
   },
   "source": [
    "We know that increasing the training (labeled) data increases model performance. Though, all the samples do not contribute equally in improving the model. Let us understand this with a few examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vceQh6w68ajb"
   },
   "source": [
    "## SVC Says: Closer is Better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGYtE9-NKfvW"
   },
   "source": [
    "We will use synthetic two-class data generated from a bivariate normal distribution for this experiment. Now, we will train a Support Vector Classifier (SVC) [<a style=\"text-decoration:none\" href=\"https://link.springer.com/article/10.1007%2FBF00994018\">8</a>] model on a subset of this dataset (5 data points) and visualize the decision boundary.\n",
    "\n",
    "(All Figures in this article are interactive. Hover over plots to know more. Click on legend items to hide/show elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POtf_3T0-OZY"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "z9kM820a-Xaz",
    "outputId": "dfcab381-73c0-408f-e1d7-f032f8c0291b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "np.random.seed(seed)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X1 = np.random.multivariate_normal([2,0],np.eye(2)*2, size=100)\n",
    "X2 = np.random.multivariate_normal([-2,0],np.eye(2)*2, size=100)\n",
    "X = np.concatenate([X1, X2])\n",
    "y = np.concatenate([np.zeros((100, 1)), np.ones((100, 1))]).squeeze()\n",
    "np.random.seed(seed)\n",
    "train_ind = np.random.choice(range(len(X)), size=5, replace=False)\n",
    "test_ind = list(set(range(len(X)))-set(train_ind))\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "C = 1.0 \n",
    "clf = svm.LinearSVC(C=C).fit(X[train_ind], y[train_ind])\n",
    "\n",
    "# get the separating hyperplane\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(-6, 4)\n",
    "yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "# plot the parallels to the separating hyperplane that pass through the\n",
    "# support vectors (margin away from hyperplane in direction\n",
    "# perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n",
    "# 2-d.\n",
    "margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n",
    "yy_down = yy - np.sqrt(1 + a ** 2) * margin\n",
    "yy_up = yy + np.sqrt(1 + a ** 2) * margin\n",
    "\n",
    "# # plot the line, the points, and the nearest vectors to the plane\n",
    "# #plt.clf()\n",
    "# plt.plot(xx, yy, 'k-',label='Decision boundary')\n",
    "# plt.plot(xx, yy_down, 'k--', label='Upper and Lower margin')\n",
    "# plt.plot(xx, yy_up, 'k--')\n",
    "# plt.ylim(-4,7)\n",
    "\n",
    "# # Plot also the training points\n",
    "# my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list('',[my_clr['y'],my_clr['b']])\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y, cmap=my_cmap, alpha=0.4)\n",
    "# plt.scatter(X[:,0][train_ind][y[train_ind]==0], \n",
    "#             X[:,1][train_ind][y[train_ind]==0], c=my_clr['y'], s=200, marker='8',\n",
    "#             label='Train points: Class-1',edgecolors='k')\n",
    "# plt.scatter(X[:,0][train_ind][y[train_ind]==1], \n",
    "#             X[:,1][train_ind][y[train_ind]==1], c=my_clr['b'], s=200, marker='8', \n",
    "#             label='Train points: Class-2',edgecolors='k')\n",
    "# arr = [1, 38, 19,191, 161, 186]\n",
    "# ii, ele = 0, arr[0]\n",
    "# plt.scatter(X[ele,0], X[ele,1], color=my_clr['y'], label='Class-1 (unlabeled)')\n",
    "# plt.annotate(chr(ii+65), (X[ele,0], X[ele,1]), size=30, color=my_clr['l_r'], xycoords='data',\n",
    "#             xytext=(X[ele,0]-1, X[ele,1]+2), textcoords='data',\n",
    "#             arrowprops=dict(arrowstyle=\"->\",\n",
    "#                             connectionstyle=\"arc3\"))\n",
    "\n",
    "# ii, ele = 1, arr[2]\n",
    "# plt.scatter(X[ele,0], X[ele,1], color=my_clr['y'])\n",
    "# plt.annotate(chr(ii+65), (X[ele,0], X[ele,1]), size=30, color=my_clr['l_r'], xycoords='data',\n",
    "#             xytext=(X[ele,0]-1, X[ele,1]+4), textcoords='data',\n",
    "#             arrowprops=dict(arrowstyle=\"->\",\n",
    "#                             connectionstyle=\"arc3\", color='k'))\n",
    "\n",
    "# ii, ele = 2, arr[3]\n",
    "# plt.scatter(X[ele,0], X[ele,1], color=my_clr['b'], label='Class-2 (unlabeled)')\n",
    "# plt.annotate(chr(ii+65), (X[ele,0], X[ele,1]), size=30, color=my_clr['l_b'], xycoords='data',\n",
    "#             xytext=(X[ele,0], X[ele,1]+3), textcoords='data',\n",
    "#             arrowprops=dict(arrowstyle=\"->\",\n",
    "#                             connectionstyle=\"arc3\"))\n",
    "\n",
    "# ii, ele = 3, arr[5]\n",
    "# plt.scatter(X[ele,0], X[ele,1], color=my_clr['b'])\n",
    "# plt.annotate(chr(ii+65), (X[ele,0], X[ele,1]), size=30, color=my_clr['l_b'], xycoords='data',\n",
    "#             xytext=(X[ele,0]-4, X[ele,1]+2), textcoords='data',\n",
    "#             arrowprops=dict(arrowstyle=\"->\",\n",
    "#                             connectionstyle=\"arc3\", color='k'))\n",
    "\n",
    "# # for i in range(150,200):\n",
    "# #     plt.annotate(str(i), (X[i,0], X[i,1]))\n",
    "\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# #plt.xlim(xx.min(), xx.max())\n",
    "# #plt.ylim(yy.min(), yy.max())\n",
    "# plt.title('Support Vector Classifier with Linear Kernel')\n",
    "# plt.legend(bbox_to_anchor=(1,0.7))\n",
    "# plt.figtext(-0.1,-0.1,'Decision boundary and margin for SVC model trained on 5 random samples',fontdict={'size':16})\n",
    "# #sns.set_context('poster')\n",
    "# plt.show()\n",
    "layout = go.Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Scatter(x=xx, y=yy,\n",
    "                    mode='lines',opacity=1,\n",
    "                    name='Decision boundary',line=dict(width=4, color='gray'), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=xx, y= yy_up,\n",
    "                    mode='lines',opacity=0.8,\n",
    "                    name='Upper margin', line=dict(width=4, color='gray',dash='dashdot'), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=xx, y= yy_down,\n",
    "                    mode='lines',opacity=0.8,\n",
    "                    name='Lower margin', line=dict(width=4, color='gray',dash='dashdot'), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=X[:, 0][test_ind][y[test_ind]==0], y=X[:, 1][test_ind][y[test_ind]==0],\n",
    "                    mode='markers',opacity=0.6,\n",
    "                    name='Unlabeled datapoints (Class 1)',\n",
    "                    marker=dict(size=6,color=px.colors.DEFAULT_PLOTLY_COLORS[0]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=X[:, 0][test_ind][y[test_ind]==1], y=X[:, 1][test_ind][y[test_ind]==1],\n",
    "                    mode='markers',opacity=0.6,\n",
    "                    name='Unlabeled datapoints (Class 2)',\n",
    "                    marker=dict(size=6,color=px.colors.DEFAULT_PLOTLY_COLORS[1]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=X[:, 0][train_ind][y[train_ind]==0], y=X[:, 1][train_ind][y[train_ind]==0],\n",
    "                    mode='markers',opacity=1,\n",
    "                    name='Train datapoints (Class 1)',\n",
    "                    marker=dict(size=12,color=px.colors.DEFAULT_PLOTLY_COLORS[0],line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=X[:, 0][train_ind][y[train_ind]==1], y=X[:, 1][train_ind][y[train_ind]==1],\n",
    "                    mode='markers',opacity=1,\n",
    "                    name='Train datapoints (Class 2)',\n",
    "                    marker=dict(size=12,color=px.colors.DEFAULT_PLOTLY_COLORS[1],line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "for ind, (i, ch, hov) in enumerate(zip([1, 191, 19, 186],['A','C','B','D'],['Far away point from confusion area',\n",
    "                                                                            'Far away point from confusion area',\n",
    "                                                                            'Closer point to confusion area',\n",
    "                                                                            'Closer point to confusion area'])):\n",
    "    fig.add_annotation(hovertext=hov,\n",
    "                x=X[i, 0],\n",
    "                y=X[i, 1],\n",
    "                text='<b>'+ch+'</b>',font = dict(size = 24, color='green'),\n",
    "                ay=-70,\n",
    "                ax=-45\n",
    "                )\n",
    "    fig.add_trace(go.Scatter(x=X[i:i+1, 0], y=X[i:i+1, 1],\n",
    "                    mode='markers',opacity=1,showlegend=False,\n",
    "                    marker=dict(size=9,color=px.colors.DEFAULT_PLOTLY_COLORS[int(ind%2)]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})')\n",
    "                )\n",
    "\n",
    "fig.update_annotations(dict(\n",
    "            showarrow=True,\n",
    "            arrowhead=7,\n",
    "))\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,\n",
    "                 zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1,\n",
    "                 #tickvals=[-4,-2,0,2,4,6],\n",
    "                 range=[-4, 6]\n",
    "                )\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,\n",
    "                 zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1,\n",
    "                 #tickvals=[-6,-4,-2,0,2,4],\n",
    "                 #range=[-6, 4]\n",
    "                )\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_layout(\n",
    "    title_text='<b>Figure 2:</b> Decision boundary and margin for Support Vector Classifier model trained on 5 random samples', \n",
    "                  title_x=0.5,\n",
    "                 xaxis_title=\"X\",\n",
    "                 yaxis_title=\"Y\"\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T0UpSLRERRw3"
   },
   "source": [
    "Support vector points help the SVC model to distinguish between various classes. The model fit in the above diagram is not accurate because we have used a small number of train datapoints. Consider some potential train points A, B, C, and D from unlabeled datapoints. Datapoints B and D are closer to the confusion area than A and C. Thus, B and D are more informative in improving the model if added to the train points. When the SVC model says, \"closer is better,\" it means closer to the confusion area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kx9MfPZST1Qx"
   },
   "source": [
    "## Confusion in Digit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3wTZ1WFUDRD"
   },
   "source": [
    "Let us consider the MNIST dataset (a well-known public dataset with labeled images of digits $0$ to $9$) for the classification task. We have shown a few examples here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "8zTBBzQY9cQh",
    "outputId": "25408a1e-0111-434f-bab8-4dc44ce691fc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "(train_pool_X, train_pool_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "train_X, Pool_X, train_y,  Pool_y = train_test_split(train_pool_X, train_pool_y, train_size=50, random_state=seed)\n",
    "\n",
    "stack = []\n",
    "for i in range(5):\n",
    "    inds = np.random.choice(np.arange(len(train_pool_X)), size=5)\n",
    "    stack.append(np.hstack(train_pool_X[inds]))\n",
    "final = np.vstack(stack)\n",
    "\n",
    "# plt.axis('off')\n",
    "# plt.imshow(final)\n",
    "# plt.figtext(0.18,0,'Figure 9: Few samples from the MNIST dataset',fontdict={'size':16});\n",
    "\n",
    "train_pool_X = train_pool_X.reshape(train_pool_X.shape[0], -1)\n",
    "train_X = train_X.reshape(train_X.shape[0], -1)\n",
    "test_X = test_X.reshape(test_X.shape[0], -1)\n",
    "Pool_X = Pool_X.reshape(Pool_X.shape[0], -1)\n",
    "\n",
    "from PIL import Image\n",
    "fig = go.Figure()\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "fig.update_xaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
    "fig.update_layout(title_text='<b>Figure 3:</b> Few random samples from the MNIST dataset',\n",
    "                  title_x=0.5,height=400,hovermode=False\n",
    "    )\n",
    "\n",
    "fig.add_layout_image(\n",
    "source=Image.fromarray(final),\n",
    "xref=\"x\",\n",
    "yref=\"y\",\n",
    "x=1.5,\n",
    "sizex=5,\n",
    "y=4,\n",
    "sizey=5,\n",
    "opacity=1,\n",
    "#layer=\"below\",\n",
    "#sizing=\"stretch\"\n",
    ")\n",
    "\n",
    "fig.layout.plot_bgcolor = '#ffffff'\n",
    "fig.layout.paper_bgcolor = '#ffffff'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the Support Vector Classifier (SVC) model on a few random samples of the MNIST dataset. Let us see what our model learns with a set of $50$ data points ($5$ samples for each class). We show the normalized confusion matrix over the test set having $10000$ samples (Figure $4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QT-pePtDUzm1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "(train_pool_X, train_pool_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "splitter = StratifiedKFold(1210)\n",
    "for _, train_ind in splitter.split(train_pool_X, train_pool_y):\n",
    "    train_X = train_pool_X[train_ind]\n",
    "    train_y = train_pool_y[train_ind]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "YrsTXWABVafG",
    "outputId": "7599a524-68f0-4a2e-fa8e-24bc583d0fd5"
   },
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(random_state=seed)\n",
    "#model = LogisticRegression(random_state=seed)\n",
    "model = SVC(random_state=seed)\n",
    "model.fit(train_X.reshape(train_X.shape[0],-1), train_y)\n",
    "pred_y = model.predict(test_X.reshape(test_X.shape[0], -1))\n",
    "acc = f1_score(test_y, pred_y,labels=[0,1,2,3,4,5,6,7,8,9], average=None)\n",
    "# fig, ax = plt.subplots()\n",
    "# format_axes(ax);\n",
    "# ax.bar(range(10), acc)\n",
    "# ax.set_xlabel('Digit')\n",
    "# #ax.grid(True)\n",
    "# ax.set_xticks(range(10))\n",
    "# ax.set_yticks(np.arange(0,1.1,0.1))\n",
    "# ax.set_ylabel('F1-score\\n(Higher is better)')\n",
    "# plt.tight_layout()\n",
    "# ax.set_title('Individual F1-score for each digit class')\n",
    "# df = pd.DataFrame(np.zeros((1, 10)), columns=['Digit_'+str(i) for i in range(10)], index=['Train Ground Truth Frequency'], dtype='int')\n",
    "# df.iloc[0] = pd.Series(train_y).value_counts().sort_index().values\n",
    "# #df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Matplotlib and Seaborn ########################\n",
    "#fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#plt.figtext(0.05,-0.02,'Confusion matrix after fitting the logistic regression model on 50 samples',\n",
    "#            fontdict={'size':16});ax.set_xlabel('Actual Digit');ax.set_ylabel('Predicted Digit');\n",
    "#sns.heatmap(np.round(CM/CM.astype(np.float).sum(axis=1),2), annot=True, ax=ax, cmap=my_cmap);\n",
    "#######################################################\n",
    "CM = confusion_matrix(test_y, pred_y)\n",
    "my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list('',[my_clr['l_b'],\n",
    "                                                                  my_clr['y'],my_clr['l_r']])\n",
    "\n",
    "# fig = px.imshow(CM, \n",
    "#                x=list(range(10)),y=list(range(10)),\n",
    "#                labels={'x':'Predicted digit','y':'Actual digit','color':'Normalized samples'},\n",
    "#                height=500)\n",
    "hover = ['All']\n",
    "CM = CM[::-1]\n",
    "Norm_CM = np.round(CM.astype('float')/CM.sum(axis=1)[:, np.newaxis],3)\n",
    "hover = [[\"Predicted digit: \"+str(i)+\\\n",
    "          '<br>Actual digit: '+str(j)+\\\n",
    "          '<br>Number of samples: '+str(CM[j, i])+\\\n",
    "          '<br>Normalized number of samples: '+str(Norm_CM[j, i])\\\n",
    "          for i in range(len(CM[0]))] for j in range(len(CM[0]))]\n",
    "fig = ff.create_annotated_heatmap(CM,colorscale=px.colors.sequential.Viridis,\n",
    "                                  x=list(range(10)),y=list(range(10)),\n",
    "                                  text=hover, hoverinfo='text', \n",
    "                                  annotation_text=Norm_CM)\n",
    "fig.update_layout(title_text='<b>Figure 4:</b> Confusion matrix over 10000 test samples after fitting an SVC model on 50 random samples', \n",
    "                  title_x=0.5,\n",
    "                 xaxis_title=\"Predicted digit\",\n",
    "                 yaxis_title=\"Actual digit\", \n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True);fig.update_xaxes(automargin=True)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,3)\n",
    "# for i,j in enumerate([19,17,9]):\n",
    "#     ax[i].set_axis_off()\n",
    "#     ax[i].imshow(test_X[j].reshape(28,28))\n",
    "# plt.figtext(0.2,0.75,'Figure 4: Digits 4, 7 and 9 have similar structure',\n",
    "#             fontdict={'size':12});\n",
    "\n",
    "fig = make_subplots(1, 3)#subplot_titles=['Walking','Running','Sitting'])\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "fig.update_xaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
    "fig.update_layout(title_text='<b>Figure 5:</b> Digits 4, 7 and 9 have similar structure',\n",
    "                  title_x=0.5,height=350, hovermode=False\n",
    "    )\n",
    "\n",
    "for ind, i in enumerate([19,17,9],1):\n",
    "    fig.add_layout_image(\n",
    "    source=Image.fromarray(test_X[i]),\n",
    "    xref=\"x\",\n",
    "    yref=\"y\",\n",
    "    x=0,\n",
    "    sizex=5,\n",
    "    y=4,\n",
    "    sizey=5,\n",
    "    opacity=1,\n",
    "    layer=\"below\",\n",
    "    #sizing=\"stretch\",\n",
    "    row=1, col=ind)\n",
    "\n",
    "fig.layout.plot_bgcolor = '#ffffff'\n",
    "fig.layout.paper_bgcolor = '#ffffff'\n",
    "#print(time()-init)\n",
    "# fig.layout.annotations[0].update(y=0.78)\n",
    "# fig.layout.annotations[1].update(y=0.78)\n",
    "# fig.layout.annotations[2].update(y=0.78)\n",
    "# fig.layout.annotations[3].update(y=0.78)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_w9wNB9Xu0P"
   },
   "source": [
    "We can see from the confusion matrix that few digits have more confusion than others. For example, the digit '8' has the least confusion; digit '9' is confused with '7' and '4'. Some digits are difficult to distinguish from the model's perspective. Thus, we may need more training examples for the before-mentioned digits to learn them correctly. Now, we will see a regression-based example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVpJVpJMd2BZ"
   },
   "source": [
    "## GP Needs 'Good' Data Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvPkqbjUJxjy"
   },
   "source": [
    "We will consider a sine curve data with added noise. We take a few samples (8 samples) as the train points, a few as the potential train points, and the rest as the test points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel, WhiteKernel\n",
    "import plotly.graph_objs as go\n",
    "from plotly.graph_objs import Layout\n",
    "\n",
    "np.random.seed(seed)\n",
    "whole_X = np.linspace(-1,1,500).reshape(-1,1)\n",
    "whole_y = np.sin(whole_X*10) + np.random.normal(size=whole_X.shape[0]).reshape(-1,1)/10\n",
    "train_ind = np.random.choice(np.arange(500), size=8,replace=False)\n",
    "np.random.seed(seed)\n",
    "pool_ind = np.linspace(0,499,20).astype(int)\n",
    "pool_X = whole_X[pool_ind]\n",
    "pool_y = whole_y[pool_ind]\n",
    "train_X = whole_X[train_ind]\n",
    "train_y = whole_y[train_ind]\n",
    "model = GaussianProcessRegressor(kernel=ConstantKernel(1)*(Matern(length_scale=1)))\n",
    "model.fit(train_X, train_y)\n",
    "pred_y, var_y = model.predict(whole_X, return_cov=True)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(whole_X, whole_y, s=10, label='Test data', color=my_clr['l_b'])\n",
    "# ax.scatter(pool_X, pool_y, marker='o', s=100, color=my_clr['y'], label='Potential train points')\n",
    "# ax.scatter(train_X, train_y, marker='o', s=100, color=my_clr['l_r'], label='Train points')\n",
    "# ax.legend(bbox_to_anchor=(1,0.5));\n",
    "# plt.figtext(0,-0.02,'Noisy sine curve dataset with train, test and potential train points',fontdict={'size':16});\n",
    "\n",
    "# Create traces\n",
    "layout = Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Scatter(x=train_X.squeeze(), y=train_y.squeeze(),\n",
    "                    mode='markers', name='Train points',marker=dict(size=12,color='black'), hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=pool_X.squeeze(), y=pool_y.squeeze(),\n",
    "                    mode='markers',\n",
    "                    name='Potential train points',marker=dict(size=12, color='rgb(240,0,0)'), hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=whole_y.squeeze(),\n",
    "                    mode='markers',opacity=0.4,\n",
    "                    name='Test points',marker=dict(size=6,color=px.colors.DEFAULT_PLOTLY_COLORS[0]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_layout(title_text='<b>Figure 6:</b> Noisy sine curve dataset with train, test and potential train points', \n",
    "                  title_x=0.5,\n",
    "                 xaxis_title=\"X\",\n",
    "                 yaxis_title=\"Y\"\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit a GPR (Gaussian Process Regressor) [11] model to our dataset with <a style=\"text-decoration:none\" href=\"https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.Matern.html\">Matern</a> kernel. GPR models additionally provide the uncertainty about the predictions. The predictive variance is measure of uncertainty of the model about its predictions (predictive mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "bTjCMxCkJ4qd",
    "outputId": "53ca030e-a84e-4d34-b4d3-21a2b8ef8964"
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel, WhiteKernel\n",
    "from plotly.graph_objs import Layout\n",
    "\n",
    "np.random.seed(seed)\n",
    "whole_X = np.linspace(-1,1,500).reshape(-1,1)\n",
    "whole_y = np.sin(whole_X*10) + np.random.normal(size=whole_X.shape[0]).reshape(-1,1)/10\n",
    "train_ind = np.random.choice(np.arange(500), size=8,replace=False)\n",
    "np.random.seed(seed)\n",
    "pool_ind = np.linspace(0,499,20).astype(int)\n",
    "pool_X = whole_X[pool_ind]\n",
    "pool_y = whole_y[pool_ind]\n",
    "train_X = whole_X[train_ind]\n",
    "train_y = whole_y[train_ind]\n",
    "model = GaussianProcessRegressor(kernel=ConstantKernel(0.1)*(Matern(length_scale=0.1)))\n",
    "# model = GaussianProcessRegressor(kernel=ConstantKernel(0.1)*(Matern(length_scale=0.1)))\n",
    "model.fit(train_X, train_y)\n",
    "pred_y, var_y = model.predict(whole_X, return_cov=True)\n",
    "rmsold = np.round(np.sqrt(np.mean((pred_y.squeeze()- whole_y.squeeze())**2)),3)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(whole_X, whole_y, s=10, label='Test data', color=my_clr['l_b'])\n",
    "# ax.plot(whole_X, pred_y, color='grey', label='Predictive mean')\n",
    "# ax.fill_between(whole_X.squeeze(), pred_y.squeeze()-var_y.diagonal(), pred_y.squeeze()+var_y.diagonal(), \n",
    "#                  alpha=0.2, label='Predictive variance',color='grey')\n",
    "# ax.scatter(pool_X, pool_y, marker='o', s=100, color=my_clr['y'], label='Potential train points')\n",
    "# ax.scatter(train_X, train_y, marker='o', s=100, color=my_clr['l_r'], label='Train points')\n",
    "# ax.legend(bbox_to_anchor=(1,0.5));\n",
    "# ii = 0\n",
    "# ax.annotate(chr(ii+65), (pool_X[5], pool_y[5]), size=25, color=my_clr['b'], xycoords='data',\n",
    "#             xytext=(pool_X[5]-0.35, pool_y[5]), textcoords='data',\n",
    "#             arrowprops=dict(arrowstyle=\"->\",\n",
    "#                             connectionstyle=\"arc3\"))\n",
    "# ii = 1; iii = 6\n",
    "# ax.annotate(chr(ii+65), (pool_X[iii], pool_y[iii]), size=25, color=my_clr['b'], xycoords='data',\n",
    "#             xytext=(pool_X[iii]-0.35, pool_y[iii]), textcoords='data',\n",
    "#             arrowprops=dict(arrowstyle=\"->\",\n",
    "#                             connectionstyle=\"arc3\"))\n",
    "# ii = 2; iii = 7\n",
    "# ax.annotate(chr(ii+65), (pool_X[iii], pool_y[iii]), size=25, color=my_clr['b'], xycoords='data',\n",
    "#             xytext=(pool_X[iii]-0.40, pool_y[iii]), textcoords='data',\n",
    "#             arrowprops=dict(arrowstyle=\"->\",\n",
    "#                             connectionstyle=\"arc3\"))\n",
    "# ii = 3; iii = 8\n",
    "# ax.annotate(chr(ii+65), (pool_X[iii], pool_y[iii]), size=25, color=my_clr['b'], xycoords='data',\n",
    "#             xytext=(pool_X[iii]-0.30, pool_y[iii]), textcoords='data',\n",
    "#             arrowprops=dict(arrowstyle=\"->\",\n",
    "#                             connectionstyle=\"arc3\"));\n",
    "# ax.set_xlabel('X');ax.set_ylabel('Y');\n",
    "# ax.set_title('RMSE: '+str(np.round(np.sqrt(np.mean((pred_y.squeeze()- whole_y.squeeze())**2)),3)));\n",
    "# plt.figtext(0.34,-0.07,'Trained GP model',fontdict={'size':16})\n",
    "# format_axes(ax);\n",
    "layout = Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=whole_y.squeeze(),\n",
    "                    mode='markers',opacity=0.4,\n",
    "                    name='Test points',marker=dict(size=6, color=px.colors.DEFAULT_PLOTLY_COLORS[0]), hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=pool_X.squeeze(), y=pool_y.squeeze(),\n",
    "                    mode='markers',\n",
    "                    name='Potential train points',marker=dict(size=12, color='rgb(240,0,0)'), \n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=train_X.squeeze(), y= train_y.squeeze(),\n",
    "                    mode='markers',\n",
    "                    name='Train points', marker=dict(size=12, color='black'), \n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=pred_y.squeeze()-var_y.diagonal(), fill='tonexty',\n",
    "                         fillcolor='rgba(128,128,128,0.2)',showlegend=False,name='Predictive variance',\n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})',\n",
    "                    mode='none' # override default markers+lines\n",
    "                    ))\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=pred_y.squeeze()+var_y.diagonal(), fill='tonexty',\n",
    "                         fillcolor='rgba(128,128,128,0.2)',\n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})',\n",
    "                    mode= 'none', name='Predictive variance'))\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y= pred_y.squeeze(),\n",
    "                    mode='lines',opacity=0.8,\n",
    "                    name='Predictive mean', line=dict(width=4, color='gray',dash='dashdot'), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "for i in ['1','2']:\n",
    "    fig.add_annotation(hovertext='Is this a good point?',\n",
    "                x=pool_X[5][0],\n",
    "                y=pool_y[5][0],\n",
    "                text='<b>A</b>',font = dict(size = 24, color='green'),ay=-50\n",
    "                )\n",
    "    fig.add_annotation(hovertext='Is this a good point?',\n",
    "                x=pool_X[6][0],\n",
    "                y=pool_y[6][0],\n",
    "                text=\"<b>B</b>\",font = dict(size = 24, color='orange'),ay=50\n",
    "                )\n",
    "    fig.add_annotation(hovertext='Is this a good point?',\n",
    "                x=pool_X[7][0],\n",
    "                y=pool_y[7][0],\n",
    "                text=\"<b>C</b>\",font = dict(size = 24, color='orange'),ay=50\n",
    "                )\n",
    "    fig.add_annotation(hovertext='Is this a good point?',\n",
    "                x=pool_X[-3][0],\n",
    "                y=pool_y[-3][0],\n",
    "                text=\"<b>D</b>\",font = dict(size = 24, color='green'),ay=-50\n",
    "                )\n",
    "fig.update_annotations(dict(\n",
    "            showarrow=True,\n",
    "            arrowhead=7,\n",
    "            ax=-5,\n",
    "))\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_layout(title_text='<b>Figure 7:</b> Trained GP model on 8 randomly selected datapoints<br>RMSE:'+str(np.round(rmsold,3)), \n",
    "                  title_x=0.5,\n",
    "                 xaxis_title=\"X\",\n",
    "                 yaxis_title=\"Y\"\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUO3y4yr7v8R"
   },
   "source": [
    "We can observe that uncertainty (predictive variance) is higher at the distant datapoints from the train points. Let us consider a set of datapoints A, B, C, and D, to see if they are equally informative to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "I5nNgxGHROxf",
    "outputId": "38091535-558c-47d2-92e9-d6871994992c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "np.random.seed(seed)\n",
    "whole_X = np.linspace(-1,1,500).reshape(-1,1)\n",
    "whole_y = np.sin(whole_X*10) + np.random.normal(size=whole_X.shape[0]).reshape(-1,1)/10\n",
    "train_ind = np.random.choice(np.arange(500), size=8,replace=False)\n",
    "np.random.seed(seed)\n",
    "pool_ind = np.linspace(0,499,20).astype(int)\n",
    "pool_X = whole_X[pool_ind]\n",
    "pool_y = whole_y[pool_ind]\n",
    "train_X = whole_X[train_ind]\n",
    "train_y = whole_y[train_ind]\n",
    "model = GaussianProcessRegressor(kernel=ConstantKernel(0.1)*(Matern(length_scale=0.1)))\n",
    "# model = GaussianProcessRegressor(kernel=ConstantKernel(0.1)*(Matern(length_scale=0.1)))\n",
    "model.fit(train_X, train_y)\n",
    "pred_y, var_y = model.predict(whole_X, return_cov=True)\n",
    "rmsold = np.round(np.sqrt(np.mean((pred_y.squeeze()- whole_y.squeeze())**2)),3)\n",
    "\n",
    "\n",
    "\n",
    "layout = Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "##########################\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=whole_y.squeeze(),\n",
    "                    mode='markers',opacity=0.4,\n",
    "                    name='Test points',marker=dict(size=6, color=px.colors.DEFAULT_PLOTLY_COLORS[0]), \n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})', visible=True))\n",
    "fig.add_trace(go.Scatter(x=pool_X.squeeze(), y=pool_y.squeeze(),\n",
    "                    mode='markers',\n",
    "                    name='Potential train points',marker=dict(size=12, color='rgb(240,0,0)'), \n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})', visible=True))\n",
    "fig.add_trace(go.Scatter(x=train_X.squeeze(), y= train_y.squeeze(),\n",
    "                    mode='markers',\n",
    "                    name='Train points', marker=dict(size=12, color='black'), \n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})', visible=True))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=pred_y.squeeze()-var_y.diagonal(), fill='tonexty',\n",
    "                         fillcolor='rgba(128,128,128,0.2)',showlegend=False, visible=True,\n",
    "                         name='Predictive variance',hovertemplate='(%{x:.2f},%{y:.2f})',\n",
    "                    mode='none' # override default markers+lines\n",
    "                    ))\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=pred_y.squeeze()+var_y.diagonal(), fill='tonexty',\n",
    "                         fillcolor='rgba(128,128,128,0.2)',hovertemplate='(%{x:.2f},%{y:.2f})',\n",
    "                    mode= 'none', name='Predictive variance'))\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y= pred_y.squeeze(),\n",
    "                    mode='lines',opacity=0.8,\n",
    "                    name='Predictive mean', line=dict(width=4, color='gray',dash='dashdot'), \n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})', visible=True))\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "rms = []\n",
    "for n_i, (n1, n2) in enumerate([[6,7], [5,-3]], 1):\n",
    "  model = GaussianProcessRegressor(kernel=ConstantKernel(0.1)*(Matern(length_scale=0.1)))\n",
    "  new_train_X = np.array(train_X.tolist() + [pool_X[n1], pool_X[n2]])\n",
    "  new_train_y = np.array(train_y.tolist() + [pool_y[n1], pool_y[n2]])\n",
    "  model.fit(new_train_X, new_train_y)\n",
    "  pred_y, var_y = model.predict(whole_X, return_cov=True)\n",
    "  rms.append(np.round(np.sqrt(np.mean((pred_y.squeeze()- whole_y.squeeze())**2)),3))\n",
    "#   ax[n_i].scatter(whole_X, whole_y, s=10, label='Test data',color=my_clr['l_b'])\n",
    "#   ax[n_i].plot(whole_X, pred_y, color='grey', label='Predictive mean')\n",
    "#   ax[n_i].fill_between(whole_X.squeeze(), pred_y.squeeze()-var_y.diagonal(), \n",
    "#                        pred_y.squeeze()+var_y.diagonal(), alpha=0.2, label='Predictive variance',color='grey')\n",
    "#   ax[n_i].scatter(pool_X, pool_y, marker='o', s=100, color=my_clr['y'], label='Potential train points')\n",
    "#   ax[n_i].scatter(new_train_X, new_train_y, marker='o', s=100, color=my_clr['l_r'], label='Train points')\n",
    "#   ii = 0\n",
    "#   ax[n_i].annotate(chr(ii+65), (pool_X[5], pool_y[5]), size=25, color=my_clr['b'], xycoords='data',\n",
    "#               xytext=(pool_X[5]-0.35, pool_y[5]), textcoords='data',\n",
    "#               arrowprops=dict(arrowstyle=\"->\",\n",
    "#                               connectionstyle=\"arc3\"))\n",
    "#   ii = 1; iii = 6\n",
    "#   ax[n_i].annotate(chr(ii+65), (pool_X[iii], pool_y[iii]), size=25, color=my_clr['b'], xycoords='data',\n",
    "#               xytext=(pool_X[iii]-0.35, pool_y[iii]), textcoords='data',\n",
    "#               arrowprops=dict(arrowstyle=\"->\",\n",
    "#                               connectionstyle=\"arc3\"))\n",
    "#   ii = 2; iii = 7\n",
    "#   ax[n_i].annotate(chr(ii+65), (pool_X[iii], pool_y[iii]), size=25, color=my_clr['b'], xycoords='data',\n",
    "#               xytext=(pool_X[iii]-0.25, pool_y[iii]), textcoords='data',\n",
    "#               arrowprops=dict(arrowstyle=\"->\",\n",
    "#                               connectionstyle=\"arc3\"))\n",
    "#   ii = 3; iii = -3\n",
    "#   ax[n_i].annotate(chr(ii+65), (pool_X[iii], pool_y[iii]), size=25, color=my_clr['b'], xycoords='data',\n",
    "#               xytext=(pool_X[iii]-0.30, pool_y[iii]), textcoords='data',\n",
    "#               arrowprops=dict(arrowstyle=\"->\",\n",
    "#                               connectionstyle=\"arc3\"));\n",
    "#################################\n",
    "  fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=whole_y.squeeze(),\n",
    "                    mode='markers',opacity=0.4,\n",
    "                    name='Test points',showlegend=True,\n",
    "                    marker=dict(size=6, color=px.colors.DEFAULT_PLOTLY_COLORS[0]), \n",
    "                           hovertemplate='(%{x:.2f},%{y:.2f})', visible=False))\n",
    "  fig.add_trace(go.Scatter(x=pool_X.squeeze(), y=pool_y.squeeze(),\n",
    "                        mode='markers',showlegend=True,\n",
    "                        name='Potential train points',marker=dict(size=12, color='rgb(240,0,0)'), \n",
    "                        hovertemplate='(%{x:.2f},%{y:.2f})', visible=False))\n",
    "  fig.add_trace(go.Scatter(x=new_train_X.squeeze(), y= new_train_y.squeeze(),\n",
    "                        mode='markers',showlegend=True,\n",
    "                        name='Train points', marker=dict(size=12, color='black'), \n",
    "                        hovertemplate='(%{x:.2f},%{y:.2f})', visible=False))\n",
    "\n",
    "  fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=pred_y.squeeze()-var_y.diagonal(), fill='tonexty',\n",
    "                             fillcolor='rgba(128,128,128,0.2)',showlegend=False,\n",
    "                        mode='none', visible=False # override default markers+lines\n",
    "                        ))\n",
    "  fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=pred_y.squeeze()+var_y.diagonal(), fill='tonexty',\n",
    "                             fillcolor='rgba(128,128,128,0.2)',showlegend=True,\n",
    "                        mode= 'none', name='Predictive variance', visible=False))\n",
    "  fig.add_trace(go.Scatter(x=whole_X.squeeze(), y= pred_y.squeeze(),\n",
    "                        mode='lines',opacity=0.8,showlegend=True,\n",
    "                        name='Predictive mean', line=dict(width=4, color='gray',dash='dashdot'), \n",
    "                           hovertemplate='(%{x:.2f},%{y:.2f})', visible=False))\n",
    "#################################\n",
    "# ax[0].set_title('Added points {B, C} to the train points\\nRMSE on the test data: '+str(rms[0]));\n",
    "# ax[1].set_title('Added points {A, D} to the train points\\nRMSE on the test data: '+str(rms[1]));\n",
    "# ax[1].legend(bbox_to_anchor=(1,0.5));\n",
    "# ax[0].set_ylabel('Y');\n",
    "# ax[1].set_yticks(())\n",
    "# ax[1].set_xlabel('X');ax[0].set_xlabel('X');\n",
    "# plt.subplots_adjust(wspace=.0);\n",
    "# plt.figtext(0.2,-0.1,'Effect on the predictions after adding a few specific points to the train points',fontdict={'size':16})\n",
    "# format_axes(ax[0]);format_axes(ax[1]);\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,\n",
    "                 zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,\n",
    "                 zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1,title_text=\"X\", side='bottom')\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "\n",
    "fig.update_layout(title_text='<b>Figure 8:</b> Trained GP model on 8 randomly selected datapoints<br>RMSE:'+str(np.round(rmsold,3)), \n",
    "                  title_x=0.5,title_y=0.95,\n",
    "                 xaxis_title=\"X\",\n",
    "                 yaxis_title=\"Y\", paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "\n",
    "for i in ['1','2']:\n",
    "    fig.add_annotation(hovertext='Good point',\n",
    "                x=pool_X[5][0],\n",
    "                y=pool_y[5][0],\n",
    "                text='<b>A</b>',font = dict(size = 24, color='green'),ay=-50\n",
    "                )\n",
    "    fig.add_annotation(hovertext='Not so good point',\n",
    "                x=pool_X[6][0],\n",
    "                y=pool_y[6][0],\n",
    "                text=\"<b>B</b>\",font = dict(size = 24, color='orange'),ay=50\n",
    "                )\n",
    "    fig.add_annotation(hovertext='Not so good point',\n",
    "                x=pool_X[7][0],\n",
    "                y=pool_y[7][0],\n",
    "                text=\"<b>C</b>\",font = dict(size = 24, color='orange'),ay=50\n",
    "                )\n",
    "    fig.add_annotation(hovertext='Good point',\n",
    "                x=pool_X[-3][0],\n",
    "                y=pool_y[-3][0],\n",
    "                text=\"<b>D</b>\",font = dict(size = 24, color='green'),ay=-50\n",
    "                )\n",
    "fig.update_annotations(dict(\n",
    "            showarrow=True,\n",
    "            arrowhead=7,\n",
    "            ax=-5,\n",
    "))\n",
    "\n",
    "updatemenus=[\n",
    "        dict(\n",
    "            type = \"buttons\",\n",
    "            direction = \"left\",\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args = [{'visible': [True]*6+[False]*6+[False]*6},\n",
    "                         {'title': '<b>Figure 7:</b> Trained GP model on 8 randomly selected datapoints<br>RMSE:'+str(np.round(rmsold,3))}],\n",
    "                    label=\"Original model\",\n",
    "                    method=\"update\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args = [{'visible': [False]*6+[True]*6+[False]*6},\n",
    "                         {'title': '<b>Figure 7:</b> Effect of adding various train points to the model<br>RMSE:'+str(np.round(rms[0],3))}],\n",
    "                    label=\"<b>Click</b> to add points B and C<br>to the train points of the Original model\",\n",
    "                    method=\"update\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args = [{'visible': [False]*6+[False]*6+[True]*6},\n",
    "                         {'title': '<b>Figure 7:</b> Effect of adding various train points to the model<br>RMSE:'+str(np.round(rms[1],3))}],\n",
    "                    label=\"<b>Click</b> to add points A and D<br>to the train points of the Original model\",\n",
    "                    method=\"update\"\n",
    "                )\n",
    "            ]),\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=0.1,\n",
    "            xanchor=\"left\",\n",
    "            y=1.12,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "fig['layout']['updatemenus'] = updatemenus\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lCwqgoAF8-No"
   },
   "source": [
    "We can say from RMSE and predictive variance that datapoints A and D are more informative to the model than B and C. Note that adding points to the train set is equivalent to annotating unlabeled data and using them for training. We can either have an intelligent way to choose these 'good' points or randomly choose some datapoints and label them. Active learning techniques can help us determine the 'good' datapoints, which are likely to improve our model. Now, we will discuss active learning techniques in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oRbxUncQtoF"
   },
   "source": [
    "# The Basics of Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMhyH0UaQzm_"
   },
   "source": [
    "Wikipedia quotes the definition of active learning as the following, \n",
    "* *'Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs.'*\n",
    "\n",
    "The below diagram illustrates the general flow of active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "jQRQIZvRmMFH",
    "outputId": "3a2eca6e-6f85-471c-9a0b-e748d723c5da",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,8),dpi=70)\n",
    "# plt.imshow(Image.open(\"../images/AL basics.png\"))\n",
    "# plt.axis('off');\n",
    "# plt.figtext(0.37,0.2,'Figure 8: General flow of Active Learning',fontdict={'size':16});\n",
    "from PIL import Image\n",
    "fig = go.Figure()\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "fig.update_xaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_xaxes(showticklabels=False).update_yaxes(showticklabels=False)\n",
    "fig.update_layout(title_text='<b>Figure 9:</b> General flow of active learning',\n",
    "                  title_x=0.5,hovermode=False\n",
    "    )\n",
    "\n",
    "fig.add_layout_image(\n",
    "source=Image.open('../images/AL basics.png'),\n",
    "xref=\"x\",\n",
    "yref=\"y\",\n",
    "x=0,\n",
    "sizex=5,\n",
    "y=4,\n",
    "sizey=6,\n",
    "opacity=1,\n",
    "#layer=\"below\",\n",
    "#sizing=\"stretch\"\n",
    ")\n",
    "\n",
    "fig.layout.plot_bgcolor = '#ffffff'\n",
    "fig.layout.paper_bgcolor = '#ffffff'\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILZo2qbSpVVV"
   },
   "source": [
    "As shown in the flow diagram, an ML model gives a few samples to the oracle (human annotator or data source) for labeling from an unlabeled pool or distribution. These samples are chosen intelligently by a few criteria. Thus, active learning is also called as optimal experimental design in other words [[link](http://eprints.sics.se/3600/)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjCeBM2bpA5p"
   },
   "source": [
    "### Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TTYHNDS9pKHN"
   },
   "source": [
    "An ML model can randomly sample datapoints and send them to the oracle for labeling. Random sampling will also eventually result in capturing the global distribution of the dataset in the train datapoints. However, active learning aims to improve the model by intelligently selecting the datapoints for labeling. Thus, Random sampling is an appropriate baseline to compare with active learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8KNvzWjsD0-"
   },
   "source": [
    "# Different Scenarios for Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "txlSER4usUPj"
   },
   "source": [
    "We have mainly three different scenarios of active learning:\n",
    "1. **Membership Query Synthesis** [12]: In this scenario, the model has an underlying distribution of data points from where it can generate the samples. The generated samples are sent to the oracle for labeling.\n",
    "1. **Stream-Based Selective Sampling** [13]: We have a live stream of online data samples, and for each incoming sample model can choose to query for it or discard it based on some criteria. One possible criterion is to have some information measure or a query strategy to query the incoming sample [<a style=\"text-decoration:none\" href=\"http://burrsettles.com/pub/settles.activelearning.pdf\">2</a>, <a style=\"text-decoration:none\" href=\"https://dl.acm.org/doi/10.5555/3091622.3091641\">3</a>]. Another way is define several hypotheses that define a region where they agree for labeled dataset called *version space* [<a style=\"text-decoration:none\" href=\"http://burrsettles.com/pub/settles.activelearning.pdf\">2</a>, <a style=\"text-decoration:none\" href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.5764&rep=rep1&type=pdf\">4</a>] but disagree for some unlabeled dataset. Calculating the exact region is expensive thus approximations and other methods are used in practice [<a style=\"text-decoration:none\" href=\"http://burrsettles.com/pub/settles.activelearning.pdf\">2</a>, <a style=\"text-decoration:none\" href=\"https://dl.acm.org/doi/10.1145/130385.130417\">5</a>, <a style=\"text-decoration:none\" href=\"https://link.springer.com/article/10.1007/BF00993277\">6</a>, <a style=\"text-decoration:none\" href=\"https://papers.nips.cc/paper/3325-a-general-agnostic-active-learning-algorithm\">7</a>].\n",
    "1. **Pool-Based Sampling** [8]: In this case, we already have a pool of unlabeled samples (We called them potential train points in the prior discussion). Based on some criteria, model queries for a few samples. \n",
    "\n",
    "The pool-based sampling scenario is suitable for most of the real-world applications. Thus, we restrict our article to pool-based sampling only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "04p00pVK14_G"
   },
   "source": [
    "# Pool-Based Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgkeldN02C1X"
   },
   "source": [
    "We can query the datapoints from an unlabeled pool with the following methods:\n",
    "1. **Uncertainty Sampling** [8]: We query the samples based on the model's uncertainty about the predictions. \n",
    "1. **Query by Committee** [5]: In this approach, we create a committee of two or more models.  The Committee queries for the samples where predictions disagree the most among themselves.\n",
    "\n",
    "We will demonstrate each of the above strategies with examples in the subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lMsLskR4YWk"
   },
   "source": [
    "## Uncertainty Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different approaches for the Classification and Regression tasks in uncertainty sampling. We will go through them one by one with examples here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZGCs-pnT8iI"
   },
   "source": [
    "### Digit Classification with MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8P74BlyB7NNd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def individual_acc(pred_yy):\n",
    "    #fig, ax = plt.subplots()\n",
    "    ind_acc = np.array([np.inf]*len(set(test_y)))\n",
    "    for each in range(len(set(test_y))):\n",
    "        ind_acc[each] = f1_score(test_y, pred_yy, labels=[each], average=None)\n",
    "        #ind_acc[each] = recall_score(test_y, pred_yy, labels=[each], average=None)\n",
    "    return ind_acc.squeeze()\n",
    "\n",
    "def overall_rmse(pred_y):\n",
    "    rmse = []\n",
    "    for preds in pred_y:\n",
    "        rmse.append(mean_squared_error(test_y, preds, squared=False))\n",
    "    return rmse\n",
    "\n",
    "def overall_acc(pred_y):\n",
    "    acc = []\n",
    "    for preds in pred_y:\n",
    "        acc.append(f1_score(test_y, preds, average='macro'))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def plot_samples(X, ax):\n",
    "    ax.set_axis_off()\n",
    "    upper = np.hstack([x.reshape(28,28) for x in X[:5]])\n",
    "    lower = np.hstack([x.reshape(28,28) for x in X[5:]])\n",
    "    final = np.vstack([upper, lower])\n",
    "    ax.imshow(final)\n",
    "    ax.set_title('Queried samples')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilwFNbvW74UZ"
   },
   "source": [
    "We will fit a Random Forest Classifier model [10] (an ensemble model consisting of multiple Decision Tree Classifiers) on a few random samples (50 samples) of MNIST dataset and visualize the predictions. We will explain different ways to perform uncertainty sampling using the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "zqOF3SAk9zS6",
    "outputId": "6f191282-a4f2-4b39-da36-20b5b45b9e36",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(seed)\n",
    "(train_pool_X, train_pool_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "train_X, Pool_X, train_y,  Pool_y = train_test_split(train_pool_X, train_pool_y, train_size=50, random_state=seed)\n",
    "\n",
    "train_pool_X = train_pool_X.reshape(train_pool_X.shape[0], -1)\n",
    "train_X = train_X.reshape(train_X.shape[0], -1)\n",
    "test_X = test_X.reshape(test_X.shape[0], -1)\n",
    "Pool_X = Pool_X.reshape(Pool_X.shape[0], -1)\n",
    "\n",
    "model = RandomForestClassifier(random_state=seed)\n",
    "#model=SVC(kernel='linear', random_state=seed, probability=True)\n",
    "#model = LogisticRegression(random_state=seed, max_iter=200)\n",
    "model.fit(train_X, train_y)\n",
    "proba = model.predict_proba(test_X)\n",
    "p = np.argsort([entropy(p) for p in proba])[::-1]\n",
    "q = np.argsort([max(p) for p in proba])[::-1]\n",
    "r = np.argsort([sorted(p)[-1]-sorted(p)[-2] for p in proba])[::-1]\n",
    "\n",
    "ii = 5000\n",
    "i_list = [p[ii], q[ii], r[ii]]\n",
    "# fig, ax = plt.subplots(3, 3, figsize=(16,7))\n",
    "# plt.subplots_adjust(wspace=0.,hspace=0.4)\n",
    "# for i,img_i in enumerate(i_list):\n",
    "#     ax[0, i].imshow(test_X[img_i].reshape(28, 28))\n",
    "#     ax[0, i].set_yticks(());ax[0, i].set_xticks(())\n",
    "#     ax[0, i].set_title('Ground Truth\\nSample '+str(i+1))\n",
    "#     ax[1, i].bar(range(10), proba[img_i])\n",
    "#     ax[1, i].set_xticks(range(10));ax[1, 0].set_yticks(np.arange(0,1,0.2));ax[1, max(1,i)].set_yticks(())\n",
    "#     ax[1, max(1,i)].set_ylim(0,1)\n",
    "#     #ax[1, i].grid(True)\n",
    "#     ax[1, i].set_xlabel('Pedicted digit');ax[1, 0].set_ylabel('Probability');\n",
    "#     l = max(proba[img_i])\n",
    "#     m = sorted(proba[img_i])[-1] - sorted(proba[img_i])[-2]\n",
    "#     e = entropy(proba[img_i])\n",
    "#     l_a = np.argmax(proba[img_i])\n",
    "#     l_b = np.argsort(proba[img_i])[-2]\n",
    "#     ax[1, i].annotate(np.round(l,2),(l_a-0.4,np.round(l,2)+0.05))\n",
    "#     ax[1, i].annotate(np.round(sorted(proba[img_i])[-2],2),(l_b-0.4,np.round(sorted(proba[img_i])[-2],2)+0.05))\n",
    "#     ax[2, i].axhline(entropy(proba[i_list[1]]), color='y')\n",
    "#     img_im = i_list[0]\n",
    "#     img_il = i_list[0]\n",
    "#     ax[2, i].axhline(max(proba[img_il]), color='g')\n",
    "#     ax[2, i].axhline(sorted(proba[img_im])[-1] - sorted(proba[img_im])[-2], color='r')\n",
    "#     ax[2, i].set_ylim(0,2.5);ax[2, max(1,i)].set_yticks(())\n",
    "#     b_l = ax[2, i].bar(['Least confident','Margin sampling','Entropy'], [l, m, e])\n",
    "#     b_l[0].set_color('g')\n",
    "#     b_l[1].set_color('r')\n",
    "#     b_l[2].set_color('y')\n",
    "#     ax[2, i].annotate(str(np.round(l,3)),(0, 1),size=18);ax[2, i].annotate(str(np.round(m, 3)),(1, 1),size=18)\n",
    "#     ax[2, i].annotate(str(np.round(e,3)),(2, 1),size=18)\n",
    "#     format_axes(ax[0, i]);format_axes(ax[1, i]);format_axes(ax[2, i]);\n",
    "#     #ax[2, i].set_xlim(0,3)\n",
    "\n",
    "# plt.figtext(0.38,+0.02,'Figure 10: Various methods of uncertainty sampling',fontdict={'size':16});\n",
    "# #plt.tight_layout();\n",
    "\n",
    "fig = make_subplots(3, 3,\n",
    "                    vertical_spacing=0.1,\n",
    "                    subplot_titles=['Ground Truth<br>Sample 1',\n",
    "                                          'Ground Truth<br>Sample 2',\n",
    "                                          'Ground Truth<br>Sample 3'])\n",
    "\n",
    "############# Common\n",
    "#fig.update_yaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "#fig.update_xaxes(automargin=True, showgrid=False, zeroline=False)\n",
    "#fig.update(layout_coloraxis_showscale=False)\n",
    "methods = ['Least confident','Margin sampling','Entropy']\n",
    "\n",
    "for ind, i in enumerate(i_list,1):\n",
    "    fig.add_layout_image(\n",
    "    source=Image.fromarray(test_X[i].reshape(28, 28)),\n",
    "    xref=\"x\",\n",
    "    yref=\"y\",\n",
    "    x=0.6,\n",
    "    sizex=6,\n",
    "    y=4,\n",
    "    sizey=5,\n",
    "    opacity=1,\n",
    "    layer=\"below\",\n",
    "    #sizing=\"stretch\",\n",
    "    row=1, col=ind)\n",
    "    fig.update_xaxes(automargin=True, showgrid=False, \n",
    "                     zeroline=False, showticklabels=False, \n",
    "                     row=1,col=ind)\n",
    "    fig.update_yaxes(automargin=True, showgrid=False, \n",
    "                     zeroline=False, showticklabels=False, \n",
    "                     row=1,col=ind)\n",
    "for ind, i in enumerate(i_list,1):\n",
    "    fig.add_trace(go.Bar(x=list(range(10)), y=proba[i],text=list(map(lambda x: \"%.2f\"%x, proba[i])),\n",
    "                  textposition='outside',\n",
    "                  marker=dict(color=plotly.colors.DEFAULT_PLOTLY_COLORS[ind]),showlegend=False,\n",
    "                  name='Predicted probabilities<br>Sample '+str(ind),\n",
    "                  hovertemplate='(%{x:d},%{y:.2f})'),row=2, col=ind)\n",
    "    fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,\n",
    "                     zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1,\n",
    "                     title=dict(text='Predicted digit',standoff=0),\n",
    "                     tickvals=list(range(10)),row=2,col=ind)\n",
    "    fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,\n",
    "                     zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1,\n",
    "                     tickvals=list(np.linspace(0,1,5)),\n",
    "                     title='Probability'*(not bool(ind-1)),\n",
    "                     range=[0,1],\n",
    "                     row=2,col=ind)\n",
    "    fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',\n",
    "                     gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "for ind, i in enumerate(i_list,1):\n",
    "    l = max(proba[i])\n",
    "    m = sorted(proba[i])[-1] - sorted(proba[i])[-2]\n",
    "    e = entropy(proba[i])\n",
    "    fig.add_trace(go.Bar(x=methods, y=[l, m, e],showlegend=False,text=[\"%.3f\"%l, \"%.3f\"%m, \"%.3f\"%e],\n",
    "                  textposition='auto',\n",
    "                  marker=dict(color=plotly.colors.DEFAULT_PLOTLY_COLORS[ind]),\n",
    "                  hovertemplate='(%{x},%{y:.2f})'),row=3, col=ind)\n",
    "    fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,\n",
    "                     zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1,\n",
    "                     row=3,col=ind)\n",
    "    fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,\n",
    "                     zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1,\n",
    "                     #tickvals=list(np.linspace(0,0.9,10)),\n",
    "                     range=[0,2],\n",
    "                     row=3,col=ind)\n",
    "    fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',\n",
    "                     gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.layout.plot_bgcolor = '#ffffff'\n",
    "fig.layout.paper_bgcolor = '#ffffff'\n",
    "fig.update_layout(title_text='<b>Figure 10:</b> Various methods of uncertainty sampling',\n",
    "                  title_x=0.5,title_y=0.97,height=600)\n",
    "\n",
    "for ind, i in enumerate(i_list,1):\n",
    "    fig.add_annotation(arrowcolor='rgba(128,128,128,0.4)',arrowwidth=0.1,\n",
    "                hovertext=\"Samples having minimum 'Max proba' value are selected<br>for annotation in 'Least confident' method.\",\n",
    "                x=np.argmax(proba[i]),y=max(proba[i]),\n",
    "                text='<b>'+'Max<br>Proba'+'</b>',font = dict(size = 12, color='grey'),\n",
    "                ay=-40,\n",
    "                ax=0,row=2,col=ind\n",
    "                )\n",
    "    fig.add_annotation(arrowcolor='rgba(128,128,128,0.4)',arrowwidth=0.1,\n",
    "                hovertext=\"This is used to calculate the margin<br>in 'Margin sampling'. method<br>margin = (Max Proba) - (2nd Max Proba)\",\n",
    "                x=np.argsort(proba[i])[-2],y=np.sort(proba[i])[-2],\n",
    "                text='<b>'+'2nd<br>Max<br>Proba'+'</b>',font = dict(size = 12, color='grey'),\n",
    "                ay=-40,\n",
    "                ax=0,row=2,col=ind\n",
    "                )\n",
    "#     fig.add_trace(go.Scatter(x=X[i:i+1, 0], y=X[i:i+1, 1],\n",
    "#                     mode='markers',opacity=1,showlegend=False,\n",
    "#                     marker=dict(size=9,color=px.colors.DEFAULT_PLOTLY_COLORS[int(ind%2)]), \n",
    "#                     hovertemplate='(%{x:.2f},%{y:.2f})'),row=2,col=ind\n",
    "#                 )\n",
    "\n",
    "#print(time()-init)\n",
    "# fig.layout.annotations[0].update(y=0.78)\n",
    "# fig.layout.annotations[1].update(y=0.78)\n",
    "# fig.layout.annotations[2].update(y=0.78)\n",
    "# fig.layout.annotations[3].update(y=0.78)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQTDfkHwP94g"
   },
   "source": [
    "Above are the model predictions in terms of probability for a few random test samples. We can use different uncertainty strategies as the following.\n",
    "1. **Least confident** [16]: In this method, we choose samples for which the most probable class's probability is minimum. In the above example, sample 1 is least confident about its highest probable class digit '1'. So, we will choose sample 1 among all for labeling using this approach.\n",
    "\n",
    "1. **Margin sampling** [17]: In this method, we choose samples for which the difference between the probability of the most probable class and the second most probable class is minimum. In the above example, sample 1 has the least margin; thus, we will choose sample 1 for labeling using this approach.\n",
    "\n",
    "1. **Entropy** [15]: Entropy can be calculated for N number of classes using the following equation, where $P(x_i)$ is predicted probability for $i^{th}$ class. \n",
    "\\begin{equation}\n",
    "H(X) = -\\sum\\limits_{i=0}^{N}P(x_i)log_2P(x_i)\n",
    "\\end{equation}\n",
    "Entropy is likely to be higher if the probability is distributed over all classes. Thus, we can say that if entropy is higher, the model is more confused among all classes.\n",
    "For the above example, sample 2 has the highest entropy in predictions. So, we can choose the same for labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YR9CQRxeUFLZ"
   },
   "source": [
    "We will now see the effect of active learning with these strategies on test data (contains 10000 samples). We will continue using the Random Forest Classifier model for this problem. We start with 50 samples as the initial train set and add 100 actively chosen samples over 100 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "riRAKQTdUnCX",
    "outputId": "fb171407-2a69-4f1b-ba9c-d97bd457d6db"
   },
   "outputs": [],
   "source": [
    "from modAL.uncertainty import uncertainty_sampling, margin_sampling, entropy_sampling\n",
    "from multiprocessing import Pool\n",
    "from IPython.display import clear_output\n",
    "np.random.seed(seed)\n",
    "N = 100\n",
    "C = 1\n",
    "\n",
    "def random_sampling(clf, X, *args):\n",
    "    ind = [np.random.choice(range(len(X)))]\n",
    "    return ind, X[ind]\n",
    "    \n",
    "# def run(x):\n",
    "#   s_i, strategy = x\n",
    "#   list_pred_y = {s_i:[]}\n",
    "#   list_new_X = {s_i:[]}\n",
    "#   Pool_X_tmp, Pool_y_tmp = Pool_X.copy(), Pool_y.copy()\n",
    "#   train_X_tmp, train_y_tmp = train_X.copy(), train_y.copy()\n",
    "#   learner = ActiveLearner(estimator=LogisticRegression(random_state=seed, max_iter=500), \n",
    "#                         query_strategy=strategy, \n",
    "#                        X_training=train_X_tmp, y_training=train_y_tmp)\n",
    "\n",
    "#   for i in range(N):\n",
    "#       print(s_i, i)\n",
    "#       clear_output(wait=True)\n",
    "#       pred_y = learner.predict(test_X)\n",
    "#       list_pred_y[s_i].append(pred_y)\n",
    "#       new_inds, new_X = learner.query(Pool_X_tmp, C)\n",
    "#       list_new_X[s_i].append(new_X)\n",
    "#       learner.teach(Pool_X_tmp[new_inds], Pool_y_tmp[new_inds])\n",
    "#       Pool_X_tmp, Pool_y_tmp = np.delete(Pool_X_tmp, new_inds, axis=0), np.delete(Pool_y_tmp, new_inds, axis=0)\n",
    "#   return (list_pred_y, list_new_X)\n",
    "\n",
    "# ans = [run((si,st)) for si,st in enumerate([uncertainty_sampling, margin_sampling, \n",
    "#                                             entropy_sampling, random_sampling])]\n",
    "# list_pred_y = {i:ans[i][0][i] for i in range(4)}\n",
    "# list_new_X = {i:ans[i][1][i] for i in range(4)}\n",
    "# pd.to_pickle(list_pred_y,'../data/list_pred_y_LR')\n",
    "# pd.to_pickle(list_new_X, '../data/list_new_X_LR')\n",
    "# clear_output()\n",
    "# from modAL.models import Committee\n",
    "# def runQBC():\n",
    "#     list_pred_y = {4:[]}\n",
    "#     list_new_X = {4:[]}\n",
    "#     Pool_X_tmp, Pool_y_tmp = Pool_X.copy(), Pool_y.copy()\n",
    "#     train_X_tmp, train_y_tmp = train_X.copy(), train_y.copy()\n",
    "#     learner1 = ActiveLearner(estimator=LogisticRegression(random_state=seed, max_iter=500),\n",
    "#                        X_training=train_X_tmp, y_training=train_y_tmp)\n",
    "#     learner2 = ActiveLearner(\n",
    "#         estimator=RandomForestClassifier(random_state=seed),\n",
    "#         X_training=train_X_tmp, y_training=train_y_tmp\n",
    "#     )\n",
    "#     learner3 = ActiveLearner(\n",
    "#         estimator=SVC(random_state=seed, probability=True),\n",
    "#         X_training=train_X_tmp, y_training=train_y_tmp\n",
    "#     )\n",
    "#     learner = Committee(learner_list=[learner1, learner2, learner3])\n",
    "\n",
    "#     for i in range(N):\n",
    "#       print(i)\n",
    "#       clear_output(wait=True)\n",
    "#       pred_y = learner.predict(test_X)\n",
    "#       list_pred_y[4].append(pred_y)\n",
    "#       new_inds, new_X = learner.query(Pool_X_tmp, C)\n",
    "#       list_new_X[4].append(new_X)\n",
    "#       learner.teach(Pool_X_tmp[new_inds], Pool_y_tmp[new_inds])\n",
    "#       Pool_X_tmp, Pool_y_tmp = np.delete(Pool_X_tmp, new_inds, axis=0), np.delete(Pool_y_tmp, new_inds, axis=0)\n",
    "#     return (list_pred_y, list_new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_pred_y_QBC, list_new_X_QBC = runQBC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_pred_y = pd.read_pickle('../data/list_pred_y_RF')\n",
    "# list_new_X = pd.read_pickle('../data/list_new_X_RF')\n",
    "# list_pred_y.update(list_pred_y_QBC)\n",
    "# list_new_X.update(list_new_X_QBC)\n",
    "# pd.to_pickle(list_new_X, '../data/list_new_X_ALL')\n",
    "# pd.to_pickle(list_pred_y, '../data/list_pred_y_ALL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "colab_type": "code",
    "id": "JA2WC_eEh2fK",
    "outputId": "0e35d82e-c800-4b4a-ef8c-ed560eca6811",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_pred_y = pd.read_pickle('../data/list_pred_y_RF')\n",
    "list_new_X = pd.read_pickle('../data/list_new_X_RF')\n",
    "\n",
    "fig = plt.figure(figsize=(13,7))\n",
    "fig.suptitle(r\"$\\bf{\" + 'Animation 1:' + \"}$\"+'Comparison among various querying methods of uncertainty sampling and random baseline')\n",
    "ax = np.empty((3,4)).astype(np.object)\n",
    "ij = 1\n",
    "for i in range(3):\n",
    "  ax[i,0] = fig.add_subplot(3, 4, ij)\n",
    "  ij += 1\n",
    "  for j in range(1,4):\n",
    "    ax[i,j] = fig.add_subplot(3, 4, ij, sharey = ax[i,0])\n",
    "    ij += 1\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "def iter(i):\n",
    "    colrs = ['r','g','b','y']\n",
    "    for s_i, s_name in enumerate(['Least confident', 'Margin sampling', 'Entropy', 'Random sampling']):\n",
    "      print(i)\n",
    "      clear_output(wait=True)\n",
    "      ax[0, s_i].cla()\n",
    "      ax[1, s_i].cla()\n",
    "      ax[2, s_i].cla()\n",
    "      ax[0, s_i].set_title(s_name, color=plotlyC(s_i, 0))\n",
    "      ax[0, s_i].set_xlabel('Digit')\n",
    "      ax[0, 0].set_ylabel('Individual F1-score')\n",
    "      ax[2, s_i].set_xlabel('Iterations')\n",
    "      ax[2, 0].set_ylabel('Overall F1-score')\n",
    "      ax[0, s_i].set_xticks(range(10))\n",
    "      ax[0, s_i].set_yticks(np.linspace(0,1,5))\n",
    "      #ax[1, s_i].set_ylim(0,28)\n",
    "      #ax[2, s_i].set_ylim(0.4, 0.6)\n",
    "      ax[0, s_i].grid(True)\n",
    "      ax[2, s_i].grid(True)\n",
    "      ax[0, s_i].set_ylim(0,1)\n",
    "      ax[0, s_i].bar(range(10), individual_acc(list_pred_y[s_i][i]), color=plotlyC(s_i, 0))\n",
    "      ax[1, s_i].imshow(list_new_X[s_i][i].reshape(28,28))\n",
    "      ax[1, s_i].set_xticks(())\n",
    "      ax[1, s_i].set_yticks(())\n",
    "      ax[1, s_i].set_xlabel('Queried sample')\n",
    "      ax[2, s_i].plot(range(1,i+1), overall_acc(list_pred_y[s_i][:i]), 'o-', color=plotlyC(s_i, 0), markersize=3)\n",
    "      ax[2, s_i].set_ylim(0.5,0.9)\n",
    "      #ax[2, s_i].legend()\n",
    "# ax[0, 0].set_title('Animation 1: Comparison among various querying methods of Uncertainty sampling and random baseline')\n",
    "plt.legend()\n",
    "plt.close()\n",
    "anim = FuncAnimation(fig, iter, frames=range(100))\n",
    "rc('animation', html='jshtml')\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFwvlewXoHbu"
   },
   "source": [
    "The above animation shows F1-scores for samples of individual digits and overall F1-scores across all digits after each iteration. We can see that each of the strategies, except random sampling, tends to choose more samples of a digit class having a lower F1-score. Margin sampling performs better than the other strategies in terms of F1-score. Margin sampling and Least confident method easily outperform the random baseline. The entropy method, in this case, is comparable to the random baseline. The Figure below shows a comparison of all strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "colab_type": "code",
    "id": "9RYM17pSpWYf",
    "outputId": "9d0ce5a9-867d-4268-a96b-54f330379339"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# for s_i, s_name in enumerate(['least_confident', 'margin_sampling', 'entropy', 'random']):\n",
    "#   ax.plot(range(1,N+1), overall_acc(list_pred_y[s_i]), 'o-',label=s_name, markersize=1)\n",
    "# ax.set_xlabel('Iterations');ax.set_ylabel('Overall F1-Score on test data')\n",
    "# ax.legend(bbox_to_anchor=(1, 0.5));\n",
    "# plt.figtext(-0.1,-0.08,'Figure 11: Comparison of overall F1-score among all strategies and random baseline',fontdict={'size':16})\n",
    "# format_axes(ax);\n",
    "\n",
    "# Create traces\n",
    "layout = Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "for s_i, s_name in enumerate(['Least confident', 'Margin sampling', 'Entropy', 'Random sampling']):\n",
    "    fig.add_trace(go.Scatter(x=list(range(1,N+1)), y=overall_acc(list_pred_y[s_i]),\n",
    "                    mode='lines+markers',\n",
    "                    name=s_name,\n",
    "                    line=dict(width=2,color=px.colors.DEFAULT_PLOTLY_COLORS[s_i]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_layout(title_text='<b>Figure 11:</b> Comparison of F1-score between uncertainty sampling methods and random sampling',\n",
    "                  title_x=0.5,\n",
    "                 xaxis_title='Iterations',\n",
    "                 yaxis_title='Overall F1-Score on test data',\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have seen uncertainty for classification tasks. Now, we see an example of regression to understand uncertainty sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression on Noisy Sine Curve "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider the sine curve dataset we have used in an earlier discussion. We will fit the Gaussian Process regressor model with Matern kernel on randomly chosen 8 data points from the noisy sine curve dataset. The uncertainty measure for the regression tasks is the standard deviation or the predictive variance. In this example, we will take predictive variance as our measure of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, ConstantKernel, WhiteKernel\n",
    "\n",
    "np.random.seed(seed)\n",
    "whole_X = np.linspace(-1,1,500).reshape(-1,1)\n",
    "whole_y = np.sin(whole_X*10) + np.random.normal(size=whole_X.shape[0]).reshape(-1,1)/10\n",
    "train_ind = np.random.choice(np.arange(500), size=8,replace=False)\n",
    "pool_ind = np.linspace(0,499,20).astype(int)\n",
    "pool_X = whole_X[pool_ind]\n",
    "pool_y = whole_y[pool_ind]\n",
    "train_X = whole_X[train_ind]\n",
    "train_y = whole_y[train_ind]\n",
    "test_X, test_y = whole_X.copy(), whole_y.copy()\n",
    "model = GaussianProcessRegressor(kernel=ConstantKernel(1)*(Matern(length_scale=0.1)))\n",
    "model.fit(train_X, train_y)\n",
    "pred_y, var_y = model.predict(whole_X, return_cov=True)\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.fill_between(whole_X.squeeze(), pred_y.squeeze()-var_y.diagonal(), \n",
    "#                  pred_y.squeeze()+var_y.diagonal(), alpha=0.2, label='Predictive variance',color='grey')\n",
    "# ax.scatter(whole_X, whole_y, label='Test set',s=10,c=my_clr['l_b'])\n",
    "# ax.scatter(pool_X, pool_y, label='Pool set',s=100,c=my_clr['y'])\n",
    "# ax.scatter(train_X, train_y, label='Train set',s=100,c=my_clr['l_r'])\n",
    "# ax.plot(whole_X, pred_y, label='Predictive mean', c='grey')\n",
    "# ax.legend(bbox_to_anchor=(1,0.5));\n",
    "# ax.set_xlabel('X');ax.set_ylabel('Y');\n",
    "# plt.figtext(0.3,-0.08,'Initial fit on the dataset',fontdict={'size':16})\n",
    "# format_axes(ax);\n",
    "\n",
    "# Create traces\n",
    "layout = Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=whole_y.squeeze(),\n",
    "                    mode='markers',opacity=0.6,\n",
    "                    name='Test points',marker=dict(size=6,color=px.colors.DEFAULT_PLOTLY_COLORS[0]), hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=pool_X.squeeze(), y=pool_y.squeeze(),\n",
    "                    mode='markers',\n",
    "                    name=\"Pool points<br>(potential train points)\",marker=dict(size=12, color='rgb(240,0,0)'), hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=train_X.squeeze(), y=train_y.squeeze(),\n",
    "                    mode='markers', name='Train points',marker=dict(size=12,color='black'), hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=pred_y.squeeze()-var_y.diagonal(), fill='tonexty',\n",
    "                         fillcolor='rgba(128,128,128,0.2)',showlegend=False,name='Predictive variance',\n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})',\n",
    "                    mode='none' # override default markers+lines\n",
    "                    ))\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y=pred_y.squeeze()+var_y.diagonal(), fill='tonexty',\n",
    "                         fillcolor='rgba(128,128,128,0.2)',\n",
    "                         hovertemplate='(%{x:.2f},%{y:.2f})',\n",
    "                    mode= 'none', name='Predictive variance'))\n",
    "fig.add_trace(go.Scatter(x=whole_X.squeeze(), y= pred_y.squeeze(),\n",
    "                    mode='lines',opacity=0.8,\n",
    "                    name='Predictive mean', line=dict(width=4, color='gray',dash='dashdot'), hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_layout(title_text='<b>Figure 12:</b> GP model fitted on 8 random datapoints of noisy sine curve dataset', \n",
    "                  title_x=0.5,\n",
    "                 xaxis_title=\"X\",\n",
    "                 yaxis_title=\"Y\"\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per uncertainty criteria, we should label the samples with higher predictive variance. Now, we will show a comparison of uncertainty sampling with random sampling for ten iterations. We also show the next sample to query at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pred_y_gp = {0:[], 1:[]}\n",
    "list_var_y_gp = {0:[], 1:[]}\n",
    "list_new_X_gp = {0:[], 1:[]}\n",
    "list_new_y_gp = {0:[], 1:[]}\n",
    "list_new_model = {0:[], 1:[]}\n",
    "np.random.seed(seed)\n",
    "N=10\n",
    "def GP_regression_std(regressor, X):\n",
    "    _, std = regressor.predict(X, return_std=True)\n",
    "    query_idx = np.argmax(std)\n",
    "    return [query_idx], X[[query_idx]]\n",
    "def GP_regression_random(regressor, X):\n",
    "    query_idx = np.random.choice(range(len(X)))\n",
    "    return [query_idx], X[[query_idx]]\n",
    "\n",
    "for s_i, q_func in enumerate([GP_regression_std, GP_regression_random]):\n",
    "    pool_X_tmp, pool_y_tmp = pool_X.copy(), pool_y.copy()\n",
    "    train_X_tmp, train_y_tmp = train_X.copy(), train_y.copy()\n",
    "    learner = ActiveLearner(\n",
    "        estimator=GaussianProcessRegressor(kernel=ConstantKernel(0.1)*(Matern(length_scale=0.1))+WhiteKernel(0.1), \n",
    "                                           random_state=seed),\n",
    "        query_strategy=q_func,\n",
    "        X_training=train_X_tmp, y_training=train_y_tmp\n",
    "    )\n",
    "    for i in range(N):\n",
    "        print(s_i, i)\n",
    "        clear_output(wait=True)\n",
    "        pred_y, var_y = learner.predict(whole_X, return_cov=True)\n",
    "        list_pred_y_gp[s_i].append(pred_y)\n",
    "        list_var_y_gp[s_i].append(var_y)\n",
    "        new_ind, new_X = learner.query(pool_X_tmp)\n",
    "        list_new_X_gp[s_i].append(new_X[0])\n",
    "        list_new_y_gp[s_i].append(pool_y_tmp[new_ind][0])\n",
    "        learner.teach(new_X, pool_y_tmp[new_ind])\n",
    "        pool_X_tmp, pool_y_tmp = np.delete(pool_X_tmp, new_ind, axis=0), np.delete(pool_y_tmp, new_ind, axis=0)\n",
    "        \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(13,7))\n",
    "fig.suptitle(r\"$\\bf{\" + 'Animation 2:' + \"}$\"+'Comparison of uncertainty sampling and random sampling on GP regression')\n",
    "plt.subplots_adjust(wspace=0.04)\n",
    "def iter_gp(i):\n",
    "    ax[0,0].cla()\n",
    "    ax[1,0].cla()\n",
    "    ax[0,1].cla()\n",
    "    ax[1,1].cla()\n",
    "    for s_i, s_name in enumerate(['Uncertainty sampling', 'Random sampling']):\n",
    "        pred_y, var_y = list_pred_y_gp[s_i][i], list_var_y_gp[s_i][i]\n",
    "        ax[0, s_i].fill_between(whole_X.squeeze(), pred_y.squeeze()-var_y.diagonal(), \n",
    "                                pred_y.squeeze()+var_y.diagonal(), alpha=0.2, label='Predictive variance')\n",
    "        ax[0, s_i].scatter(pool_X, pool_y,s=10,c='g')\n",
    "        ax[0, s_i].plot(whole_X, pred_y,label='Predictive mean')\n",
    "        ax[0, s_i].scatter(whole_X, whole_y,s=10,c=my_clr['l_b'],label='Test data')\n",
    "        ax[0, s_i].set_ylim(-1.5,3);ax[0, 1].set_yticks(())\n",
    "        ax[0, s_i].set_xlabel('X');ax[0, 0].set_ylabel('Y');\n",
    "        ax[0, s_i].set_title(s_name)\n",
    "        #ax[1, s_i].grid(True)\n",
    "        ax[1, s_i].plot(range(1,i+2), overall_rmse(list_pred_y_gp[s_i][:i+1]), 'o-', label=s_name)\n",
    "#         for x_iii, yyy in enumerate(overall_rmse(list_pred_y_gp[s_i][:i]), 1):\n",
    "#             ax[1, s_i].annotate(np.round(yyy,2), (x_iii-0.5, yyy+0.5*((-1)**x_iii)))\n",
    "        ax[1, s_i].set_xlabel('Iterations');ax[1, 0].set_ylabel('RMSE');\n",
    "        ax[0, s_i].scatter(train_X.tolist()+list_new_X_gp[s_i][:i], y=train_y.tolist()+list_new_y_gp[s_i][:i], \n",
    "                           s=100, c='k',\n",
    "                           label='Train points')\n",
    "        ax[0, s_i].scatter(list_new_X_gp[s_i][i], list_new_y_gp[s_i][i], \n",
    "                           label='Next sample to query', s=100,c='tab:red')\n",
    "        ax[1, s_i].set_xticks(np.arange(0,N+1,2))\n",
    "        ax[1, s_i].set_ylim(0.1, 0.65);ax[1, 1].set_yticks(());\n",
    "        format_axes(ax[1, s_i]);\n",
    "        format_axes(ax[0, s_i]);\n",
    "    ax[0, 1].legend(loc='upper left',prop={'size':10});\n",
    "        #ax[1, s_i].set_yticks(np.linspace(-1.5,2,11))\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.close()\n",
    "anim = FuncAnimation(fig, iter_gp, frames=range(N))\n",
    "rc('animation', html='jshtml')\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animation 2 demonstrates a comparison between uncertainty sampling and random sampling. We can observe that uncertainty sampling-based samples are more informative to the model and ultimately help reduce model uncertainty (variance) and RMSE compared to random sampling. \n",
    "\n",
    "Now, we will discuss the query by committee method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by Committee (QBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query by committee approach involves creating a committee of two or more learners or models. Each of the learners can vote for samples in the pool set. Samples for which all committee members disagree the most are considered for querying. For classification tasks, we can take a mode of votes from all learners, and in regression settings, we can take average predictions from all the learners. The central intuition behind QBC is to minimize the <a style=\"text-decoration:none\" href=\"http://www2.cs.uregina.ca/~dbd/cs831/notes/ml/vspace/3_vspace.html#:~:text=A%20version%20space%20is%20a,remembering%20any%20of%20the%20examples.\">*version space*</a>. Initially, each model has different hypotheses that try to converge as we query more samples.\n",
    "\n",
    "We can set up a committee for the QBC using the following approaches\n",
    "1. Same model with different hyperparameters\n",
    "1. Same model with different segments of the dataset\n",
    "1. Different models with the same dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qn7VdX467NNj"
   },
   "source": [
    "## Classification on Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explain the first approach (Same model with different hyperparameters) using SVC (Support Vector Classifier) model with <a style=\"text-decoration:none\" href=\"https://en.wikipedia.org/wiki/Radial_basis_function_kernel\">RBF</a> kernel on Iris dataset.\n",
    "\n",
    "We initially train the model on six samples and Actively choose 30 samples from the pool set. We will test the model performance at each iteration on the same test set of 30 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "IR = load_iris()\n",
    "X = IR['data'][:,:2]\n",
    "y = IR['target']\n",
    "t_names = IR['target_names']\n",
    "f_names = IR['feature_names']\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.scatter(X[:,0][y==0],X[:,1][y==0], label=t_names[0], c=my_clr['y'])\n",
    "# ax.scatter(X[:,0][y==1],X[:,1][y==1], label=t_names[1], c=my_clr['l_b'])\n",
    "# ax.scatter(X[:,0][y==2],X[:,1][y==2], label=t_names[2], c=my_clr['l_r'])\n",
    "# ax.legend(bbox_to_anchor=(1,0.5));plt.xlabel(f_names[0]);\n",
    "# ax.set_ylabel(f_names[1]);\n",
    "# plt.figtext(0.4,-0.1,'Iris dataset',fontdict={'size':16});\n",
    "# format_axes(ax);\n",
    "aa,bb,cc=0.5,1,10\n",
    "\n",
    "# Create traces\n",
    "layout = Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "for i in range(3):\n",
    "    fig.add_trace(go.Scatter(x=X[:,0][y==i],y=X[:,1][y==i],\n",
    "                    mode='markers',\n",
    "                    name='Iris '+t_names[i].capitalize(),marker=dict(size=6,color=px.colors.DEFAULT_PLOTLY_COLORS[i]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_layout(title_text='<b>Figure 13:</b> Iris dataset',\n",
    "                  title_x=0.5,\n",
    "                 xaxis_title=f_names[0].capitalize(),\n",
    "                 yaxis_title=f_names[1].capitalize(),\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib.colors import ListedColormap\n",
    "from modAL import Committee\n",
    "\n",
    "cmap_light = ListedColormap([plotlyC(0,10),\n",
    "                             plotlyC(1,10), \n",
    "                             plotlyC(2,10)])\n",
    "train_pool_X, test_X, train_pool_y, test_y = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "train_X, pool_X, train_y, pool_y = train_test_split(train_pool_X, train_pool_y, train_size=6, random_state=seed)\n",
    "#clf = KNeighborsClassifier(n_neighbors=6, weights='distance')\n",
    "#clf = SVC()\n",
    "#clf.fit(train_X, train_y)\n",
    "\n",
    "### dec\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "h = 0.02\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "def get_Z(clf):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    Z[0,0] = 0\n",
    "    Z[Z.shape[0]-1,Z.shape[1]-1] = 1\n",
    "    Z[0,1] = 2\n",
    "    return Z\n",
    "\n",
    "def plot_decision(Z, ax, i):\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "    ax.scatter(pool_X[:, 0], pool_X[:, 1], c=['white'],edgecolor='k',\n",
    "                     s=20, label='Pool set')\n",
    "    ax.scatter(np.array(list_new_x_iris)[:i, 0],np.array(list_new_x_iris)[:i, 1],\n",
    "               c=[plotlyC(0,0) if list_new_y_iris[j]==0 \\\n",
    "               else plotlyC(1,0) if list_new_y_iris[j]==1 \\\n",
    "               else plotlyC(2,0) for j in range(i)],\n",
    "               s=20)\n",
    "    ax.scatter(np.array(list_new_x_iris)[i:i+1, 0],np.array(list_new_x_iris)[i:i+1, 1], \n",
    "               c=[plotlyC(0,0)] if list_new_y_iris[i]==0 \\\n",
    "               else [plotlyC(1,0)] if list_new_y_iris[i]==1 \\\n",
    "               else [plotlyC(2,0)], \n",
    "               #cmap=cmap_bold,\n",
    "               edgecolor='k', s=100, label='Point to query')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# al = lambda x: ActiveLearner(estimator=KNeighborsClassifier(n_neighbors=x, weights='distance'),\n",
    "#                        X_training=train_X, y_training=train_y)\n",
    "al = lambda x: ActiveLearner(estimator=SVC(C=x, probability=True, random_state=seed, kernel='rbf'),\n",
    "                        X_training=train_X, y_training=train_y)\n",
    "\n",
    "committee = Committee(learner_list=[al(aa), al(bb), al(cc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pred_y_iris = {i:[] for i in range(3)}\n",
    "list_pred_all_iris = []\n",
    "list_Z_iris = {i:[] for i in range(3)}\n",
    "list_new_x_iris = []\n",
    "list_new_y_iris = []\n",
    "N = 30\n",
    "committee.fit(train_X, train_y)\n",
    "pool_X_tmp, pool_y_tmp = pool_X.copy(), pool_y.copy()\n",
    "for i in range(N):\n",
    "    print(i)\n",
    "    clear_output(wait=True)\n",
    "    list_pred_all_iris.append(committee.predict(test_X))\n",
    "    for l_i, learner in enumerate(committee):\n",
    "        list_pred_y_iris[l_i].append(learner.predict(test_X))\n",
    "        list_Z_iris[l_i].append(get_Z(learner))\n",
    "    new_ind, new_X = committee.query(pool_X_tmp, 1)\n",
    "    list_new_x_iris.append(new_X[0])\n",
    "    list_new_y_iris.append(pool_y_tmp[new_ind][0])\n",
    "    committee.teach(new_X, pool_y_tmp[new_ind])\n",
    "    pool_X_tmp, pool_y_tmp = np.delete(pool_X_tmp, new_ind, axis=0), np.delete(pool_y_tmp, new_ind, axis=0)\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_pred_all_iris = []\n",
    "rand_new_x_iris = []\n",
    "rand_new_y_iris = []\n",
    "al = lambda x: ActiveLearner(estimator=SVC(C=x, probability=True, random_state=seed),\n",
    "                        X_training=train_X, y_training=train_y)\n",
    "np.random.seed(seed)\n",
    "committee = Committee(learner_list=[al(aa), al(bb), al(cc)])\n",
    "\n",
    "#N = 20\n",
    "committee.fit(train_X, train_y)\n",
    "pool_X_tmp, pool_y_tmp = pool_X.copy(), pool_y.copy()\n",
    "for i in range(N):\n",
    "    print(i)\n",
    "    clear_output(wait=True)\n",
    "    rand_pred_all_iris.append(committee.predict(test_X))\n",
    "    new_ind = [np.random.choice(range(len(pool_X_tmp)))]\n",
    "    new_X = pool_X_tmp[new_ind]\n",
    "    rand_new_x_iris.append(new_X[0])\n",
    "    rand_new_y_iris.append(pool_y_tmp[new_ind[0]])\n",
    "    committee.teach(new_X, pool_y_tmp[new_ind])\n",
    "    pool_X_tmp, pool_y_tmp = np.delete(pool_X_tmp, new_ind, axis=0), np.delete(pool_y_tmp, new_ind, axis=0)\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "fig = plt.figure(figsize=(13,7))\n",
    "fig.suptitle(r\"$\\bf{\" + 'Animation 3:' + \"}$\"+'QBC is choosing points with maximum disagreement across learners')\n",
    "ax = np.zeros((2,3)).astype(np.object)\n",
    "ij = 1\n",
    "for i in range(2):\n",
    "    ax[i, 0]=fig.add_subplot(2,3,ij);ij+=1;\n",
    "    ax[i, 1] = fig.add_subplot(2,3,ij);ij+=1;\n",
    "    ax[i, 2] = fig.add_subplot(2,3,ij);ij+=1;\n",
    "plt.subplots_adjust(wspace=0.,hspace=0.3)\n",
    "C_l = [aa,bb,cc]\n",
    "def iter_iris(i):\n",
    "    for ii in range(3):\n",
    "        for jj in range(2):\n",
    "            ax[jj,ii].cla()\n",
    "            format_axes(ax[jj, ii]);\n",
    "        plot_decision(list_Z_iris[ii][i], ax[0,ii],i)\n",
    "        #ax[0,ii].annotate('Current',(list_new_x_iris[i-1][0], list_new_x_iris[i-1][1]))\n",
    "        ax[0,ii].set_xlabel(f_names[0]); ax[0,0].set_ylabel(f_names[1]);\n",
    "        ax[0,ii].set_title('SVC with C: '+str(C_l[ii]))\n",
    "        #ax[1,ii].grid(True)\n",
    "        b_l = ax[1,ii].bar(t_names, individual_acc(list_pred_y_iris[ii][i]))\n",
    "        #ax[1,ii].set_title(str(individual_acc(list_pred_y_iris[ii][i])))\n",
    "        b_l[0].set_color(plotlyC(0,0))\n",
    "        b_l[1].set_color(plotlyC(1,0))\n",
    "        b_l[2].set_color(plotlyC(2,0))\n",
    "        \n",
    "        ax[0, 0].legend(loc='lower right')\n",
    "        ax[1, ii].set_ylim(0,1)\n",
    "        ax[0, ii].set_ylim(0.5,5.5)\n",
    "        ax[1, 0].set_yticks(np.linspace(0,1,11))\n",
    "        ax[1, 1].set_yticks(())\n",
    "        ax[1, 2].set_yticks(())\n",
    "        ax[0, 1].set_yticks(())\n",
    "        ax[0, 2].set_yticks(())\n",
    "        ax[1, 0].set_ylabel('F1-score on test set')\n",
    "        #ax[1, 0].legend(['F1-score on test set'])\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.close()\n",
    "anim = FuncAnimation(fig, iter_iris, frames=range(N))\n",
    "rc('animation', html='jshtml')\n",
    "clear_output()\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation boundaries between different colors are decision boundaries in Animation 3. Points queried by the committee are the points where the learners disagree the most. This can be observed from the above plot. We can see that initially, all models learn different decision boundaries for the same data. Iteratively they converge to a similar hypothesis and thus start learning similar decision boundaries.\n",
    "\n",
    "We now show the comparison of the overall F1-score between random sampling and our model. QBC, most of the time, outperforms the random sampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12,4))\n",
    "# ax.plot(range(1,1+len(rand_pred_all_iris)), overall_acc(rand_pred_all_iris), label='Random baseline', color=my_clr['l_b'])\n",
    "# ax.plot(range(1,1+len(rand_pred_all_iris)), overall_acc(list_pred_all_iris), label='QBC',color=my_clr['l_r'])\n",
    "# ax.legend();ax.set_xlabel('Iterations');ax.set_ylabel('Overall F1-score');\n",
    "# ax.set_ylim(0,1)\n",
    "# plt.figtext(0.2,-0.1,'Comparison between QBC and random baseline on Iris dataset',fontdict={'size':16});\n",
    "# format_axes(ax);\n",
    "\n",
    "# plt.xticks(np.arange(1,1+len(rand_pred_all_iris),2));\n",
    "\n",
    "# Create traces\n",
    "layout = Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "fig.add_trace(go.Scatter(x=list(range(1,1+len(list_pred_all_iris))), y=overall_acc(list_pred_all_iris),\n",
    "                    mode='lines+markers',\n",
    "                    name='Query by committee',\n",
    "                    line=dict(width=2,color=px.colors.DEFAULT_PLOTLY_COLORS[1]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "fig.add_trace(go.Scatter(x=list(range(1,1+len(rand_pred_all_iris))), y=overall_acc(rand_pred_all_iris),\n",
    "                    mode='lines+markers',\n",
    "                    name='Random sampling',\n",
    "                    line=dict(width=2,color=px.colors.DEFAULT_PLOTLY_COLORS[0]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',\n",
    "                 gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',\n",
    "                 zerolinewidth=1,tickvals=list(range(1,31)))\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_layout(title_text='<b>Figure 14:</b> Comparison between QBC and Random baseline on Iris dataset',\n",
    "                  title_x=0.5,\n",
    "                 xaxis_title='Iterations',\n",
    "                 yaxis_title='Overall F1-score',\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between Uncertainty sampling and QBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we have seen and understood various active learning strategies by examples. Now, let us compare the uncertainty sampling methods and query by committee (QBC).\n",
    "\n",
    "We will use MNIST dataset to demonstrate the performance of various sampling techniques. For uncertainty sampling, we will use Random Forest classifier. For QBC, let us use three different classifiers (Random Forest Classifier, Logistic Regression [14], and Support Vector Classifier). Animation 4 shows the simulation of active learning for 100 iterations and testing F1-score on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "(train_pool_X, train_pool_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "train_X, Pool_X, train_y,  Pool_y = train_test_split(train_pool_X, train_pool_y, train_size=50, random_state=seed)\n",
    "\n",
    "stack = []\n",
    "for i in range(5):\n",
    "    inds = np.random.choice(np.arange(len(train_pool_X)), size=5)\n",
    "    stack.append(np.hstack(train_pool_X[inds]))\n",
    "final = np.vstack(stack)\n",
    "\n",
    "# plt.axis('off')\n",
    "# plt.imshow(final)\n",
    "# plt.figtext(0.18,0,'Figure 9: Few samples from the MNIST dataset',fontdict={'size':16});\n",
    "\n",
    "train_pool_X = train_pool_X.reshape(train_pool_X.shape[0], -1)\n",
    "train_X = train_X.reshape(train_X.shape[0], -1)\n",
    "test_X = test_X.reshape(test_X.shape[0], -1)\n",
    "Pool_X = Pool_X.reshape(Pool_X.shape[0], -1)\n",
    "\n",
    "list_pred_y = pd.read_pickle('../data/list_pred_y_ALL')\n",
    "list_new_X = pd.read_pickle('../data/list_new_X_ALL')\n",
    "N = 100\n",
    "## Replacing QBC with random\n",
    "list_pred_y[3] = list_pred_y[4]\n",
    "list_new_X[3] = list_new_X[4]\n",
    "\n",
    "fig = plt.figure(figsize=(13,7))\n",
    "fig.suptitle(r\"$\\bf{\" + 'Animation 4:' + \"}$\"+'Comparison among all active learning techniques')\n",
    "ax = np.empty((3,4)).astype(np.object)\n",
    "ij = 1\n",
    "for i in range(3):\n",
    "  ax[i,0] = fig.add_subplot(3, 4, ij)\n",
    "  ij += 1\n",
    "  for j in range(1,4):\n",
    "    ax[i,j] = fig.add_subplot(3, 4, ij, sharey = ax[i,0])\n",
    "    ij += 1\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "def iter(i):\n",
    "    colrs = ['r','g','b','y']\n",
    "    for s_i, s_name in enumerate(['Least confident', 'Margin sampling', 'Entropy', 'QBC']):\n",
    "      print(i)\n",
    "      clear_output(wait=True)\n",
    "      ax[0, s_i].cla()\n",
    "      ax[1, s_i].cla()\n",
    "      ax[2, s_i].cla()\n",
    "      ax[0, s_i].set_title(s_name, color=plotlyC(s_i,0))\n",
    "      ax[0, s_i].set_xlabel('Digit')\n",
    "      ax[0, 0].set_ylabel('Individual F1-score')\n",
    "      ax[2, s_i].set_xlabel('Iterations')\n",
    "      ax[2, 0].set_ylabel('Overall F1-score')\n",
    "      ax[0, s_i].set_xticks(range(10))\n",
    "      ax[0, s_i].set_yticks(np.linspace(0,1,5))\n",
    "      #ax[1, s_i].set_ylim(0,28)\n",
    "      #ax[2, s_i].set_ylim(0.4, 0.6)\n",
    "      ax[0, s_i].grid(True)\n",
    "      ax[2, s_i].grid(True)\n",
    "      ax[0, s_i].set_ylim(0,1)\n",
    "      ax[0, s_i].bar(range(10), individual_acc(list_pred_y[s_i][i]), color=plotlyC(s_i,0))\n",
    "      ax[1, s_i].imshow(list_new_X[s_i][i].reshape(28,28))\n",
    "      ax[1, s_i].set_xticks(())\n",
    "      ax[1, s_i].set_yticks(())\n",
    "      ax[1, s_i].set_xlabel('Queried sample')\n",
    "      ax[2, s_i].plot(range(1,i+1), overall_acc(list_pred_y[s_i][:i]), 'o-', color=plotlyC(s_i,0), markersize=3)\n",
    "      ax[2, s_i].set_ylim(0.5,0.9)\n",
    "      #ax[2, s_i].legend()\n",
    "      \n",
    "plt.legend()\n",
    "\n",
    "plt.close()\n",
    "anim = FuncAnimation(fig, iter, frames=range(N))\n",
    "rc('animation', html='jshtml')\n",
    "anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# for s_i, s_name in enumerate(['least_confident', 'margin_sampling', 'entropy', 'random']):\n",
    "#   ax.plot(range(1,N+1), overall_acc(list_pred_y[s_i]), 'o-',label=s_name, markersize=1)\n",
    "# ax.set_xlabel('Iterations');ax.set_ylabel('Overall F1-Score on test data')\n",
    "# ax.legend(bbox_to_anchor=(1, 0.5));\n",
    "# plt.figtext(-0.1,-0.08,'Figure 11: Comparison of overall F1-score among all strategies and random baseline',fontdict={'size':16})\n",
    "# format_axes(ax);\n",
    "N = 100\n",
    "# Create traces\n",
    "layout = Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "for s_i, s_name in enumerate(['Least confident', 'Margin sampling', 'Entropy', 'QBC']):\n",
    "    fig.add_trace(go.Scatter(x=list(range(1,N+1)), y=overall_acc(list_pred_y[s_i]),\n",
    "                    mode='lines+markers',\n",
    "                    name=s_name,\n",
    "                    line=dict(width=2,color=px.colors.DEFAULT_PLOTLY_COLORS[s_i]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'))\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update(layout_coloraxis_showscale=False)\n",
    "fig.update_layout(title_text='<b>Figure 11:</b> Comparison of overall F1-score among all active learning strategies',\n",
    "                  title_x=0.5,\n",
    "                 xaxis_title='Iterations',\n",
    "                 yaxis_title='Overall F1-Score on test data',\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query by committee is performing better than uncertainty sampling. The reason is that uncertainty sampling tends to be biased towards the actual learner, and it may miss important examples that are not in sight of the estimator [1, 5]. QBC overcomes this problem by taking votes from different models on the same datapoints or different datapoints with the same model. Our case was different models with the same datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many samples to query at once?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now, in the article, we have queried only one sample at a time. We should consider the time to retrain the model over the train set and evaluate it on the pool set. Indeed, updating the model after each queried sample is ideal as the informativeness of samples is updated without adding noise in the form of non-informative samples. Let us assume that each sample's annotation cost is constant, and thus, we can ignore it here. We will now see the effect of selecting $ K $ samples at once on model improvement and overall time-taken for the train-test process.\n",
    "\n",
    "We will use query by committee strategy to query for 30 samples on IRIS dataset. The mean results of repeating each experiment 50 times with different train, validation, test splits are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "IR = load_iris()\n",
    "t_names = IR['target_names']\n",
    "f_names = IR['feature_names']\n",
    "\n",
    "def GetF1AndTime(K, rs):\n",
    "    IR = load_iris()\n",
    "    X = IR['data'][:,:2]\n",
    "    y = IR['target']\n",
    "    train_pool_X, test_X, train_pool_y, test_y = train_test_split(X, y, test_size=0.2, random_state=rs)\n",
    "    train_X, pool_X, train_y, pool_y = train_test_split(train_pool_X, train_pool_y, train_size=5, random_state=0)\n",
    "    pool_X_tmp, pool_y_tmp = pool_X.copy(), pool_y.copy()\n",
    "    al = lambda x: ActiveLearner(estimator=SVC(C=x, probability=True, random_state=seed, kernel='rbf'),\n",
    "                        X_training=train_X, y_training=train_y)\n",
    "    committee = Committee(learner_list=[al(aa), al(bb), al(cc)])\n",
    "    N = 30\n",
    "    init = time()\n",
    "    committee.fit(train_X, train_y)\n",
    "    i = N\n",
    "    while i>0:\n",
    "        print('K:',K,'query:',i)\n",
    "        clear_output(wait=True)\n",
    "#         list_pred_all_iris.append(committee.predict(test_X))\n",
    "#         for l_i, learner in enumerate(committee):\n",
    "#             list_pred_y_iris[l_i].append(learner.predict(test_X))\n",
    "#             list_Z_iris[l_i].append(get_Z(learner))\n",
    "        new_ind, new_X = committee.query(pool_X_tmp, min(i,K))\n",
    "        #list_new_x_iris.append(new_X[0])\n",
    "        #list_new_y_iris.append(pool_y_tmp[new_ind][0])\n",
    "        committee.teach(new_X, pool_y_tmp[new_ind])\n",
    "        pool_X_tmp, pool_y_tmp = np.delete(pool_X_tmp, new_ind, axis=0), np.delete(pool_y_tmp, new_ind, axis=0)\n",
    "        i -= K\n",
    "        \n",
    "    pred_last = committee.predict(test_X)\n",
    "    return (time()-init, f1_score(test_y, pred_last, average='macro'))\n",
    "\n",
    "# (train_pool_X, train_pool_y), (test_X, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "# train_pool_X = train_pool_X.reshape(train_pool_X.shape[0], -1)\n",
    "# test_X = test_X.reshape(test_X.shape[0], -1)\n",
    "\n",
    "\n",
    "##################\n",
    "# Very time consuming\n",
    "##################\n",
    "# def GetF1andTimeMNIST(K, rs):\n",
    "#     train_X, Pool_X, train_y,  Pool_y = train_test_split(train_pool_X, train_pool_y, train_size=50, \n",
    "#                                                          random_state=rs)\n",
    "#     train_X = train_X.reshape(train_X.shape[0], -1)\n",
    "#     Pool_X = Pool_X.reshape(Pool_X.shape[0], -1)\n",
    "#     Pool_X_tmp, Pool_y_tmp = Pool_X.copy(), Pool_y.copy()\n",
    "#     train_X_tmp, train_y_tmp = train_X.copy(), train_y.copy()\n",
    "    \n",
    "#     learner1 = ActiveLearner(estimator=LogisticRegression(C=0.5, random_state=seed, max_iter=500),\n",
    "#                        X_training=train_X_tmp, y_training=train_y_tmp)\n",
    "#     learner2 = ActiveLearner(estimator=LogisticRegression(C=1, random_state=seed, max_iter=500),\n",
    "#                        X_training=train_X_tmp, y_training=train_y_tmp)\n",
    "#     learner3 = ActiveLearner(estimator=LogisticRegression(C=10, random_state=seed, max_iter=500),\n",
    "#                        X_training=train_X_tmp, y_training=train_y_tmp)\n",
    "    \n",
    "#     learner = Committee(learner_list=[learner1, learner2, learner3])\n",
    "\n",
    "#     i = 100\n",
    "#     while i>0:\n",
    "#         print('K:',K,'query:',i)\n",
    "#         clear_output(wait=True)\n",
    "#         new_inds, new_X = learner.query(Pool_X_tmp, min(i, K))\n",
    "#         learner.teach(Pool_X_tmp[new_inds], Pool_y_tmp[new_inds])\n",
    "#         Pool_X_tmp, Pool_y_tmp = np.delete(Pool_X_tmp, new_inds, axis=0), np.delete(Pool_y_tmp, new_inds, axis=0)\n",
    "#         i -= K\n",
    "#     pred_last = committee.predict(test_X)\n",
    "#     return (time()-init, f1_score(test_y, pred_last, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = []\n",
    "for K in range(1,11):    \n",
    "    rep = []\n",
    "    for n_repeat in range(50):\n",
    "        rep.append(GetF1AndTime(K, n_repeat))\n",
    "    XY.append(np.mean(np.array(rep), axis=0))\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XYN = np.array(XY)\n",
    "# Create traces\n",
    "layout = Layout(\n",
    "    paper_bgcolor='rgb(255,255,255)',\n",
    "    plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Scatter(x=list(range(1,11)), y=XYN[:,1],\n",
    "                    mode='lines+markers',\n",
    "                    name='Macro F1-score',\n",
    "                    line=dict(width=2,color=px.colors.DEFAULT_PLOTLY_COLORS[0]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'),secondary_y=True)\n",
    "fig.add_trace(go.Scatter(x=list(range(1,11)), y=XYN[:,0],\n",
    "                    mode='lines+markers',\n",
    "                    name='Time taken to query 30 samples<br>by querying K samples at once',\n",
    "                    line=dict(width=2,color=px.colors.DEFAULT_PLOTLY_COLORS[1]), \n",
    "                    hovertemplate='(%{x:.2f},%{y:.2f})'),secondary_y=False)\n",
    "\n",
    "############# Common\n",
    "fig.update_yaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',\n",
    "                 zerolinewidth=1,secondary_y=True,title_text='Macro F1-score<br>(Higher is better)')\n",
    "fig.update_xaxes(automargin=True,gridcolor='rgba(128,128,128,0.2)',gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',\n",
    "                 zerolinewidth=1)\n",
    "fig.update_yaxes(title_text=\"Time (in seconds)\", secondary_y=False,automargin=True,gridcolor='rgba(128,128,128,0.2)',\n",
    "                 gridwidth=1,zerolinecolor='rgba(128,128,128,0.2)',zerolinewidth=1)\n",
    "fig.update(layout_coloraxis_showscale=False,layout=layout)\n",
    "fig.update_layout(title_text='<b>Figure 15:</b> Trade-off between time and performance while queriying for K samples at once',\n",
    "                  title_x=0.5,\n",
    "                 xaxis_title='K',\n",
    "                #font=dict(family=\"Courier New\")\n",
    "                 )\n",
    "fig['layout']['xaxis'].update(side='bottom')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, as K increases, time taken to complete the task decreases. Macro averaged F1-score also descreases with K. From this experiment, we can conclude that a good trade-off between time and K should be choosen to achieve the optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few More Active Learning Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few more active learning techniques we are not covering in this article, but we describe them in brief here:\n",
    "1. Expected model change: Selecting the samples that would have the most significant change in the model.\n",
    "1. Expected error reduction: Selecting the samples likely to reduce the generalization error of the model.\n",
    "1. Variance reduction: Selecting samples that may help reduce output variance.\n",
    "\n",
    "With this, we end the visual tour to the active learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Settles, Burr. Active learning literature survey. University of Wisconsin-Madison Department of Computer Sciences, 2009.\n",
    "1. Danka, Tivadar, and Peter Horvath. \"modAL: A modular active learning framework for Python.\" arXiv preprint arXiv:1805.00979, 2018.\n",
    "1. Dagan and S. Engelson. Committee-based sampling for training probabilistic classifiers. In Proceedings of the International Conference on Machine Learning (ICML), pages 150157. Morgan Kaufmann, 1995.\n",
    "1. T. Mitchell. Generalization as search. Artificial Intelligence, 18:203226, 1982.\n",
    "1. H.S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In Proceedings of the ACM Workshop on Computational Learning Theory, pages 287294, 1992.\n",
    "1. D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning, 15(2):201221, 1994.\n",
    "1. S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In Advances in Neural Information Processing Systems (NIPS), volume 20, pages 353360. MIT Press, 2008.\n",
    "1. D. Lewis and W. Gale. A sequential algorithm for training text classifiers. In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval, pages 312. ACM/Springer, 1994.\n",
    "1. Imran, Ali, et al. \"AI4COVID-19: AI enabled preliminary diagnosis for COVID-19 from cough samples via an app.\" arXiv preprint arXiv:2004.01275, 2020.\n",
    "1. Breiman, Leo. \"Random forests.\" Machine learning 45.1 (2001): 5-32.\n",
    "1. Carl Eduard Rasmussen and Christopher K.I. Williams, Gaussian Processes for Machine Learning, MIT Press 2006.\n",
    "1. D. Angluin. Queries and concept learning. Machine Learning, 2:319342, 1988.\n",
    "1. D. Cohn, L. Atlas, R. Ladner, M. El-Sharkawi, R. Marks II, M. Aggoune, and D. Park. Training connectionist networks with queries and selective sampling. In Advances in Neural Information Processing Systems (NIPS). Morgan Kaufmann, 1990.\n",
    "1. Yu, Hsiang-Fu, Fang-Lan Huang, and Chih-Jen Lin. \"Dual coordinate descent methods for logistic regression and maximum entropy models.\" Machine Learning 85.1-2 (2011): 41-75.\n",
    "1. C.E. Shannon. A mathematical theory of communication. Bell System Technical Journal, 27:379423,623656, 1948.\n",
    "1. D. Lewis and J. Catlett. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 148156. Morgan Kaufmann, 1994.\n",
    "1. T. Scheffer, C. Decomain, and S. Wrobel. Active hidden Markov models for information extraction. In Proceedings of the International Conference on Advances in Intelligent Data Analysis (CAIDA), pages 309318. Springer-Verlag, 2001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VRhUlYCt7NNv",
    "5IgLt8gP7NOS"
   ],
   "name": "AL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
